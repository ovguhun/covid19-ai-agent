**Understanding the Task at Hand**

I'm focusing on the rewrite. The core goal is clear: refactor the Jupyter Notebook code, integrating earlier suggestions. I'll maintain the notebook structure, including markdown and code cells, ensuring the RAG functionality remains intact. The plan is to break down the process into manageable steps, addressing improvements sequentially for clarity and efficiency.


**Analyzing the Core Elements**

I've reviewed the notebook structure, outlining markdown and code cells. My current priority is to address the user's suggestions. I'm now focusing on integrating improvements, specifically to `similarity_top_k`, fixing formatting errors, and adding `huggingface_hub` imports. Further, I am planning to refine error messages for robustness.


**Outlining the Implementation Plan**

I've solidified the rewrite plan, breaking it down cell by cell. I'm starting with the "Hugging Face Hub Setup" section, focusing on integrating the `notebook_login` for a more Pythonic and Colab-integrated approach. The following step will be dataset download and loading.


**Structuring the Implementation Steps**

I'm now mapping the rewrite into cell-by-cell implementation. My plan is to start with the Hugging Face Hub setup, moving onto dataset download. I'll integrate explicit `huggingface_hub` imports and replace `!huggingface-cli login` with `notebook_login()`, aligning with Pythonic best practices. I'm checking the robustness of the DataFrame initialization to handle potential empty states for graceful execution.


**Mapping Out the Process**

I'm now developing the step-by-step plan for the rewrite. The focus is on integrating the `SentenceSplitter` for improved chunking and fixing formatting errors. Additionally, the implementation includes `huggingface_hub` imports and refinement of the robustness, especially error handling for empty data frames, as specified. I'm also planning on refining prompt templates.


**Planning the Code Refactoring**

I'm reviewing the detailed implementation plan, cell-by-cell. I'll start with the "Hugging Face Hub Setup," then move to data loading and preprocessing, ensuring robustness in handling empty DataFrames. I'll also update  `similarity_top_k` in the text. I plan to integrate LlamaIndex's `SentenceSplitter`.


**Finalizing Code Implementation**

I'm now finalizing the implementation, with a focus on prompt templates and ensuring all formatting errors are resolved. The cell-by-cell rewrite includes the Hugging Face setup, along with data loading. I'll integrate LlamaIndex's SentenceSplitter, making sure that the existing RAG functionality remains. I've also updated the `similarity_top_k`.


**Implementing Stepwise Improvements**

I'm now focused on the "Hugging Face Hub Setup" and the data loading sections. I'm explicitly importing `notebook_login` and replacing the CLI login with its Pythonic alternative. I've also added a check for an empty DataFrame. Furthermore, I'm getting ready to incorporate LlamaIndex's `SentenceSplitter` in the upcoming step.


Okay, I will rewrite the notebook step-by-step, incorporating the suggested improvements for clarity, efficiency, and best practices.

I'll present this as if it were a new Jupyter/Colab notebook you're creating, with each step in its own markdown and code cell.

---

```json
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Agent: Enhanced RAG with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an LLM-based AI Research Agent for the CORD-19 Dataset.\n",
    "\n",
    "---\n",
    "\n",
    "The CORD-19 (COVID-19 Open Research Dataset) is a comprehensive collection of scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses.\n",
    "This project aims to build an AI agent that specializes in analyzing this dataset to answer questions about the relationship between COVID-19 and smoking (including cigarettes, vaping, and tobacco).\n",
    "The agent will leverage a Large Language Model (LLM) and Retrieval Augmented Generation (RAG) to provide insights based on the scientific literature within CORD-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install All Necessary Libraries\n",
    "\n",
    "---\n",
    "\n",
    "The `!pip install` commands will install all the required Python packages for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install all required libraries\n",
    "!pip install gradio\n",
    "!pip install llama-index llama-index-embeddings-huggingface llama-index-llms-huggingface\n",
    "!pip install pandas pyarrow tqdm\n",
    "!pip install transformers accelerate bitsandbytes torch\n",
    "!pip install huggingface_hub  # Explicitly install for notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Verify GPU Setup\n",
    "\n",
    "---\n",
    "\n",
    "This step verifies that PyTorch can see and use the CUDA-enabled GPU. A GPU is crucial for efficient model inference and embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\") # 0 refers to the first GPU\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available. Check Colab runtime settings (Runtime -> Change runtime type -> Hardware accelerator to GPU).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hugging Face Hub Setup\n",
    "\n",
    "---\n",
    "\n",
    "This code snippet uses the `notebook_login` function to securely connect your Colab notebook to your Hugging Face account.\n",
    "This authentication is necessary to download the dataset and the language model we'll be using later.\n",
    "You'll be prompted to enter an access token from your Hugging Face account settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "try:\n",
    "    notebook_login()\n",
    "    print(\"\\n✅ Successfully logged into Hugging Face.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error logging into Hugging Face: {e}\")\n",
    "    print(\"Please ensure you have a valid Hugging Face token and retry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Google Drive Integration\n",
    "\n",
    "---\n",
    "\n",
    "This code mounts your Google Drive to the Colab environment.\n",
    "This allows you to save important files, like the vector index we will create later, directly to your personal Google Drive.\n",
    "This prevents data loss if your Colab session disconnects and saves you from having to rebuild the index every time you open the notebook.\n",
    "You'll be asked to authorize Colab to access your Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Acquisition & Preprocessing\n",
    "\n",
    "## 5.1 Download and Load the Dataset\n",
    "\n",
    "---\n",
    "\n",
    "This code downloads the CORD-19 abstracts dataset from Hugging Face and loads it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Attempting to load the CORD-19 abstracts dataset from Hugging Face...\")\n",
    "\n",
    "try:\n",
    "    # This reads the dataset directly into a pandas DataFrame.\n",
    "    # It might take a moment to download.\n",
    "    df_cord19_abstracts = pd.read_parquet(\"hf://datasets/pritamdeka/cord-19-abstract/data/train-00000-of-00001.parquet\")\n",
    "    print(\"\\n✅ Successfully loaded dataset.\")\n",
    "    print(f\"The dataset has {len(df_cord19_abstracts)} abstracts.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error loading dataset from Hugging Face: {e}\")\n",
    "    print(\"Please ensure you are logged in with 'notebook_login()' and have network access.\")\n",
    "    # Create an empty DataFrame with the expected column to prevent downstream errors\n",
    "    df_cord19_abstracts = pd.DataFrame(columns=['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Inspect and Clean Data\n",
    "\n",
    "---\n",
    "\n",
    "This code inspects the loaded data to understand its structure. `.info()` provides a technical summary (columns, data types), and `.head()` shows the first 5 rows to give us a look at the actual content.\n",
    "We then perform basic cleaning by removing any rows that might have an empty abstract and ensuring the abstract column is treated as text, which prevents errors in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the structure of the loaded data and perform basic cleaning.\n",
    "\n",
    "if 'df_cord19_abstracts' in locals() and not df_cord19_abstracts.empty:\n",
    "    print(\"--- Dataset Information ---\")\n",
    "    df_cord19_abstracts.info()\n",
    "\n",
    "    print(\"\\n\\n--- First 5 Rows of the Dataset ---\")\n",
    "    # Using display() in Colab provides a nicer table format\n",
    "    display(df_cord19_abstracts.head())\n",
    "\n",
    "    # --- Data Cleaning ---\n",
    "    print(\"\\n\\n--- Cleaning Data ---\")\n",
    "    # Remove rows where the 'abstract' column is empty or missing\n",
    "    initial_count = len(df_cord19_abstracts)\n",
    "    df_cord19_abstracts.dropna(subset=['abstract'], inplace=True)\n",
    "    # Ensure the abstract column is treated as the 'string' data type\n",
    "    df_cord19_abstracts['abstract'] = df_cord19_abstracts['abstract'].astype(str)\n",
    "\n",
    "    cleaned_count = len(df_cord19_abstracts)\n",
    "    print(f\"✅ Data cleaned: Removed {initial_count - cleaned_count} rows with empty abstracts and ensured text format.\")\n",
    "    print(f\"The dataset now has {cleaned_count} abstracts after cleaning.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ DataFrame 'df_cord19_abstracts' was not loaded correctly in the previous step. Please check if you loaded the dataset successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Keyword-based Filtering\n",
    "\n",
    "---\n",
    "\n",
    "This code snippet filters our large DataFrame of abstracts to create a smaller, more focused one.\n",
    "First, it defines a keywords list containing terms related to smoking.\n",
    "It then uses pandas' powerful `str.contains()` function to search the 'abstract' column in a case-insensitive way for any of these keywords.\n",
    "The result is a new DataFrame, `df_relevant_abstracts`, containing only the documents relevant to our research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only abstracts containing specific keywords.\n",
    "\n",
    "if 'df_cord19_abstracts' in locals() and not df_cord19_abstracts.empty:\n",
    "    print(\"--- Filtering Abstracts by Keywords ---\")\n",
    "\n",
    "    # Define your keywords related to smoking\n",
    "    keywords = [\"smoking\", \"cigarette\", \"nicotine\", \"vaping\", \"tobacco\", \"e-cigarette\", \"smoker\"]\n",
    "    print(f\"Keywords for filtering: {', '.join(keywords)}\")\n",
    "\n",
    "    # Create a search pattern: \"keyword1|keyword2|keyword3\" which means \"keyword1 OR keyword2 OR keyword3\"\n",
    "    search_terms_pattern = '|'.join(keywords)\n",
    "\n",
    "    # Filter the DataFrame and create a new one with only relevant abstracts\n",
    "    df_relevant_abstracts = df_cord19_abstracts[df_cord19_abstracts['abstract'].str.contains(search_terms_pattern, case=False, na=False)]\n",
    "\n",
    "    print(f\"\\n✅ Found {len(df_relevant_abstracts)} relevant abstracts after keyword filtering.\")\n",
    "\n",
    "    # Display the first few relevant abstracts to confirm the filtering worked\n",
    "    if len(df_relevant_abstracts) > 0:\n",
    "        print(\"\\n--- First 5 Relevant Abstracts ---\")\n",
    "        display(df_relevant_abstracts.head())\n",
    "    else:\n",
    "        print(\"\\n⚠️ Warning: No relevant abstracts were found for the given keywords.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ DataFrame 'df_cord19_abstracts' was not found or is empty. Please run the previous steps first.\")\n",
    "    # Create an empty DataFrame to prevent errors in later cells\n",
    "    df_relevant_abstracts = pd.DataFrame(columns=['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Vector Database Creation & Management\n",
    "\n",
    "## 6.1 Configure LlamaIndex Global Settings\n",
    "\n",
    "---\n",
    "\n",
    "This code snippet configures LlamaIndex's global settings.\n",
    "It specifies which model to use for creating vector embeddings (`sentence-transformers/all-MiniLM-L6-v2`)\n",
    "and explicitly tells LlamaIndex to use the GPU (`cuda`) for this process, which significantly speeds it up.\n",
    "The `Settings.llm` is set to `None` for now, as we only need the embedding model in this phase.\n",
    "\n",
    "**Note:** `embed_model` and `llm` configurations will be explicitly defined as global constants in a later step for better code organization, but we set them here for immediate use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "print(\"--- Configuring LlamaIndex Settings for Embedding Model ---\")\n",
    "\n",
    "# Set the embedding model to be used for converting text to vectors.\n",
    "# 'all-MiniLM-L6-v2' is a popular and efficient model for this.\n",
    "# 'device=\"cuda\"' ensures the GPU is used for this computationally intensive task.\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# We are not using a Large Language Model (LLM) in this phase,\n",
    "# so we set it to None in the global settings for now.\n",
    "Settings.llm = None\n",
    "\n",
    "print(\"\\n✅ Embedding model configured to run on CUDA.\")\n",
    "print(\"  LlamaIndex will now use 'all-MiniLM-L6-v2' for creating text embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Prepare Documents and Chunking (Improved)\n",
    "\n",
    "---\n",
    "\n",
    "This code takes the filtered abstracts (from `df_relevant_abstracts`) and transforms them into LlamaIndex `Document` objects.\n",
    "Crucially, instead of simple word-based splitting, we now use LlamaIndex's `SentenceSplitter`.\n",
    "This method chunks the text into smaller, semantically coherent units (sentences or groups of sentences) while trying to respect natural language boundaries.\n",
    "This improved chunking strategy allows the AI to pinpoint very specific pieces of information within the larger abstracts when searching, leading to better retrieval quality.\n",
    "A progress bar (`tqdm`) will show the status as it processes the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from tqdm import tqdm # For displaying a progress bar\n",
    "\n",
    "# This list will hold all our chunked Document objects\n",
    "documents = []\n",
    "\n",
    "if 'df_relevant_abstracts' in locals() and not df_relevant_abstracts.empty:\n",
    "    print(f\"--- Preparing and chunking {len(df_relevant_abstracts)} relevant abstracts ---\")\n",
    "\n",
    "    # Convert each abstract into a LlamaIndex Document object (initially one document per abstract)\n",
    "    # Metadata can be added here if needed, but not strictly required for basic RAG.\n",
    "    initial_documents = [Document(text=text) for text in df_relevant_abstracts['abstract'].tolist()]\n",
    "\n",
    "    # Define the SentenceSplitter to create smaller, semantically coherent chunks.\n",
    "    # chunk_size is in tokens (not words for SentenceTransformer models), 150 words is roughly 200-250 tokens.\n",
    "    # A chunk_size of 512 is typical, but 256 for abstracts ensures focused chunks.\n",
    "    # chunk_overlap ensures continuity between chunks.\n",
    "    splitter = SentenceSplitter(chunk_size=256, chunk_overlap=20)\n",
    "\n",
    "    # Process initial documents to get our final list of smaller chunks (nodes)\n",
    "    print(\"Applying SentenceSplitter to create nodes...\")\n",
    "    # The splitter directly returns nodes ready for indexing\n",
    "    nodes = splitter.get_nodes_from_documents(initial_documents, show_progress=True)\n",
    "\n",
    "    # Convert nodes back to Document objects if `VectorStoreIndex.from_documents` expects them,\n",
    "    # or directly use nodes if `VectorStoreIndex` supports `nodes` argument.\n",
    "    # For simplicity, we'll assign to `documents` ready for `from_documents`.\n",
    "    documents = nodes\n",
    "\n",
    "    print(f\"\\n✅ Created {len(documents)} document chunks (nodes) from the relevant abstracts using SentenceSplitter.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No relevant abstracts found ('df_relevant_abstracts' is missing or empty).\")\n",
    "    print(\"  Please ensure 'Keyword-based Filtering' ran successfully and found some abstracts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Create and Persist VectorStoreIndex\n",
    "\n",
    "---\n",
    "\n",
    "This code snippet takes the document chunks we just created and uses the configured embedding model (`all-MiniLM-L6-v2` on the GPU)\n",
    "to convert each chunk into a numerical vector.\n",
    "These vectors are then stored in a `VectorStoreIndex`, which is an optimized data structure that allows for very fast similarity searches.\n",
    "The code also \"persists\" (saves) this index to a specified directory (either on your Google Drive if mounted, or in Colab's temporary storage)\n",
    "so it can be reloaded later without rebuilding. This step will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "import os # To check for Google Drive path and create directories\n",
    "\n",
    "print(\"--- Creating and Persisting VectorStoreIndex ---\")\n",
    "\n",
    "# Initialize the index variable\n",
    "index = None\n",
    "\n",
    "# --- Define the directory where the index will be saved ---\n",
    "# Option 1: Google Drive (Recommended for persistence)\n",
    "# IMPORTANT: Replace 'CORD19_Smoking_Chatbot_Index' with your desired folder name in MyDrive.\n",
    "drive_persist_dir = \"/content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\"\n",
    "\n",
    "# Option 2: Local Colab storage (index will be lost if Colab session ends)\n",
    "colab_local_persist_dir = \"storage_cord19_smoking_index_colab\"\n",
    "\n",
    "# Determine the persist_dir based on whether Google Drive is mounted\n",
    "persist_dir = \"\"\n",
    "if os.path.exists(\"/content/drive/MyDrive/\"): # Check if the base MyDrive folder exists\n",
    "    persist_dir = drive_persist_dir\n",
    "    print(f\"Google Drive detected. Index will be saved to: {persist_dir}\")\n",
    "else:\n",
    "    persist_dir = colab_local_persist_dir\n",
    "    print(f\"Google Drive not detected or not accessible at '/content/drive/MyDrive/'.\")\n",
    "    print(f\"Index will be saved to local Colab storage: {persist_dir}\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "try:\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "    print(f\"Ensured directory exists: {persist_dir}\")\n",
    "except OSError as e:\n",
    "    print(f\"❌ Error creating directory {persist_dir}: {e}. Please check the path and permissions.\")\n",
    "\n",
    "# Check if the 'documents' list exists and has content\n",
    "if 'documents' in locals() and documents:\n",
    "    print(f\"\\nCreating vector index from {len(documents)} document chunks...\")\n",
    "    print(\"This process will use the GPU and may take 5-15 minutes. Please be patient.\")\n",
    "\n",
    "    # This is the core step: generate embeddings and build the index.\n",
    "    # LlamaIndex will use the 'Settings.embed_model' we configured previously.\n",
    "    # Note: `from_documents` can take a list of `Document` objects (our `documents` list (nodes)) directly.\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        show_progress=True # Displays a progress bar\n",
    "    )\n",
    "\n",
    "    # Save the created index to the specified 'persist_dir'\n",
    "    index.storage_context.persist(persist_dir=persist_dir)\n",
    "    print(f\"\\n✅ VectorStoreIndex created and successfully saved to: {persist_dir}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n❌ No document chunks found ('documents' list is missing or empty).\")\n",
    "    print(\"  Cannot create the VectorStoreIndex. Please ensure 'Prepare Documents and Chunking (Improved)' ran successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Load Index from Storage\n",
    "\n",
    "---\n",
    "\n",
    "This code snippet checks if a previously saved vector index exists in the specified directory (`persist_dir`).\n",
    "If the index was just created in the current session, it won't try to reload it.\n",
    "However, if this is a new Colab session and the `index` object doesn't exist yet, this code will load the index from disk (from your Google Drive or local Colab storage, depending on where it was saved).\n",
    "This avoids the time-consuming process of rebuilding the index every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
     "id": "slK4gXh63_vk"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "import os\n",
    "\n",
    "print(\"--- Attempting to Load Index from Storage ---\")\n",
    "\n",
    "# Define persist_dir if it's not already in memory (e.g., if restarting Colab session)\n",
    "if 'persist_dir' not in locals() or not persist_dir:\n",
    "    drive_path = \"/content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\"\n",
    "    colab_local_path = \"storage_cord19_smoking_index_colab\"\n",
    "    if os.path.exists(drive_path):\n",
    "        persist_dir = drive_path\n",
    "        print(f\"  'persist_dir' not found. Defaulting to Google Drive path: {persist_dir}\")\n",
    "    else:\n",
    "        persist_dir = colab_local_path\n",
    "        print(f\"  'persist_dir' not found. Defaulting to local Colab path: {persist_dir}\")\n",
    "\n",
    "print(f\"Checking for existing index in: {persist_dir}\")\n",
    "\n",
    "# We try to load the index if:\n",
    "# 1. The 'index' variable doesn't already exist OR it exists but is None.\n",
    "# 2. The 'persist_dir' (the directory where the index should be saved) actually exists.\n",
    "if ('index' not in locals() or index is None) and os.path.exists(persist_dir):\n",
    "    print(f\"Found existing index directory at {persist_dir}. Attempting to load index...\")\n",
    "    try:\n",
    "        # Prepare the storage context pointing to the directory.\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "        # Load the index. This will re-populate the 'index' variable.\n",
    "        index = load_index_from_storage(storage_context)\n",
    "        print(\"✅ Index loaded successfully from storage.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading index from storage {persist_dir}: {e}\")\n",
    "        print(\"  The saved index might be corrupted, or the path might be incorrect.\")\n",
    "        print(\"  You might need to rebuild the index by re-running 'Create and Persist VectorStoreIndex'.\")\n",
    "        index = None # Ensure index is None if loading failed\n",
    "\n",
    "elif 'index' in locals() and index is not None:\n",
    "    # This case means the index was already created or loaded in the current session.\n",
    "    print(\"ℹ️ Index object already exists in this session (likely created in previous step). No need to reload.\")\n",
    "\n",
    "else:\n",
    "    # This case means 'persist_dir' does not exist, and 'index' is not already populated.\n",
    "    print(f\"ℹ️ No existing index found at {persist_dir}.\")\n",
    "    print(\"  If this is your first time running through all steps, this is normal (index was just created). \")\n",
    "    print(\"  If you expected to load an index from a previous session, ensure 'persist_dir' is correct and that \")\n",
    "    print(\"  you successfully saved the index in that previous session.\")\n",
    "\n",
    "# Final check to see if the 'index' object is now available for use\n",
    "if 'index' in locals() and index is not None:\n",
    "    print(\"\\n👍 Index object is available and ready for use.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Index object is NOT available. Subsequent steps requiring the index may fail.\")\n",
    "    print(\"  Please review the messages above to diagnose the issue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. AI Agent Development\n",
    "\n",
    "## 7.1 Define Global Constants for Models and Prompts\n",
    "\n",
    "---\n",
    "\n",
    "To improve maintainability and avoid code duplication, we're defining our LLM model, embedding model, and prompt templates as global constants.\n",
    "This makes it easy to change models or prompt instructions in one place and have the changes reflect throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "print(\"--- Defining Global Constants for Models and Prompts ---\")\n",
    "\n",
    "# --- Embedding Model ---#\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# --- LLM Configuration ---#\n",
    "LLM_MODEL_NAME = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "LLM_CONTEXT_WINDOW = 2048  # Max tokens the model can see in total\n",
    "LLM_MAX_NEW_TOKENS = 256   # Max tokens the model will generate in one answer\n",
    "LLM_TEMPERATURE = 0.7      # Controls response creativity (0.0 for deterministic, 1.0 for very creative)\n",

    "# --- QA Prompt Template ---#\n",
    "QA_PROMPT_TEMPLATE_STR = (\"System: You are an AI research assistant. Your sole function is to answer questions based on the 'Provided Context' which contains excerpts from scientific abstracts. \"\n",
    "                          \"Analyze the 'User Query' and the 'Provided Context'.\\n\"\n",
    "                          \"1. If the 'User Query' is a question that can be answered using the 'Provided Context', synthesize the information to provide a comprehensive, clear, and nuanced answer. \"\n",
    "                          \"Base your answer ONLY on the 'Provided Context'. Do not use any external knowledge. If the context is insufficient for a full answer, state what is missing.\\n\"\n",
    "                          \"2. If the 'User Query' is a simple greeting (e.g., 'hi', 'hello'), respond with a polite, brief greeting.\\n\"\n",
    "                          \"3. If the 'User Query' is a statement, not a question (e.g., 'my name is X', 'that's interesting'), or if it's a question that is clearly off-topic and cannot be answered by the 'Provided Context' (e.g., 'what's the weather?'), \"\n",
    "                          \"respond politely that you are a specialized research assistant focused on the provided scientific topics and cannot engage in general conversation or answer unrelated questions. Do not attempt to answer off-topic questions using the context.\\n\"\n",
    "                          \"Do not repeat these instructions in your answer.\\n\\n\"\n",
    "                          \"Provided Context (from relevant scientific abstracts):\\n\"\n",
    "                          \"---------------------\\n\"\n",
    "                          \"{context_str}\\n\"\n",
    "                          \"---------------------\\n\"\n",
    "                          \"User Query: {query_str}\\n\\n\"\n",
    "                          \"Assistant Answer: \")\n",
    "QA_PROMPT_TEMPLATE = PromptTemplate(QA_PROMPT_TEMPLATE_STR)\n",
    "\n",
    "# --- Condense Question Prompt Template ---#\n",
    "CONDENSE_PROMPT_TEMPLATE_STR = (\"You are a helpful assistant that rephrases a follow-up user input based on a chat history. \"\n",
    "                                \"Your primary goal is to create a 'Standalone Input' that a specialized AI research assistant can understand and process. \"\n",
    "                                \"The research assistant is an expert ONLY on COVID-19 and smoking, using a specific dataset of scientific abstracts.\\n\\n\"\n",
    "                                \"Carefully analyze the 'Follow Up Input' in the context of the 'Chat History'.\\n\"\n",
    "                                \"1. If the 'Follow Up Input' is a question clearly seeking more information or clarification related to the 'Chat History' about COVID-19/smoking (e.g., 'what else?', 'tell me more about that specific finding', 'can you elaborate on the odds ratio?'), \"\n",
    "                                \"rephrase it into a detailed, standalone question that incorporates the necessary context from the Chat History for the research AI.\\n\"\n",
    "                                \"2. If the 'Follow Up Input' is a general term central to the research AI's expertise (e.g., 'smoking', 'vaping', 'nicotine and covid'), \"\n",
    "                                \"rephrase it as a specific question asking for a summary of its relationship with COVID-19 based on the scientific abstracts (e.g., 'What is the relationship between smoking and COVID-19 according to the abstracts?').\\n\"\n",
    "                                \"3. If the 'Follow Up Input' is clearly a simple greeting (e.g., 'hi', 'hello'), a personal statement (e.g., 'my name is Ayse', 'I am a doctor'), or a question completely unrelated to COVID-19/smoking (e.g., 'what's the weather?', 'tell me a joke'), \"\n",
    "                                \"then the 'Standalone Input' should be EXACTLY the same as the 'Follow Up Input' without any modification or rephrasing.\\n\\n\"\n",
    "                                \"Chat History:\\n\"\n",
    "                                \"{chat_history}\\n\\n\"\n",
    "                                \"Follow Up Input: {question}\\n\\n\"\n",
    "                                \"Standalone Input: \")\n",
    "\n",
    "CONDENSE_PROMPT_TEMPLATE = PromptTemplate(CONDENSE_PROMPT_TEMPLATE_STR)\n",
    "\n",
    "print(\"✅ Global constants for models and prompts defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Configure the Large Language Model (LLM)\n",
    "\n",
    "---\n",
    "\n",
    "This code snippet configures and loads the specific Large Language Model (LLM) that our agent will use as its \"brain.\"\n",
    "We're using `unsloth/llama-3-8b-Instruct-bnb-4bit`, a 4-bit quantized version of Llama 3 8B, which offers a good balance of performance (speed) and quality.\n",
    "The code specifies the model name, tokenizer, context window size, maximum new tokens to generate, and ensures it runs on the GPU using 16-bit precision for efficiency.\n",
    "This configured LLM is then set as the global default for LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "import torch # For torch_dtype\n",
    "\n",
    "print(\"--- Configuring the Large Language Model (LLM) ---\")\n",
    "\n",
    "if 'LLM_MODEL_NAME' not in locals():\n",
    "    print(\"❌ LLM constants not defined. Please run 'Define Global Constants for Models and Prompts' first.\")\n",
    "    # Attempt to define minimal constants for fallback\n",
    "    LLM_MODEL_NAME = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "    LLM_CONTEXT_WINDOW = 2048\n",
    "    LLM_MAX_NEW_TOKENS = 256\n",
    "    LLM_TEMPERATURE = 0.7\n",
    "\n",
    "# Configure and load the Unsloth Quantized Llama 3 8B model\n",
    "try:\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=LLM_MODEL_NAME,\n",
    "        tokenizer_name=LLM_MODEL_NAME,\n",
    "        context_window=LLM_CONTEXT_WINDOW, # Max tokens the model can see in total\n",
    "        max_new_tokens=LLM_MAX_NEW_TOKENS, # Max tokens the model will generate in one answer\n",
    "        device_map=\"auto\", # Automatically use the GPU\n",
    "        model_kwargs={\"torch_dtype\": torch.float16}, # Optimized for Unsloth models\n",
    "        generate_kwargs={\"temperature\": LLM_TEMPERATURE, \"do_sample\": True} # Controls response creativity\n",
    "    )\n",
    "    Settings.llm = llm # Set this as the global LLM for LlamaIndex\n",
    "    print(f\"\\n✅ LLM ({LLM_MODEL_NAME}) configured successfully.\")\n",
    "    print(\"  It's set as the default LLM in LlamaIndex Settings and will run on the GPU.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ CRITICAL ERROR: Failed to configure LLM. Error: {e}\")\n",
    "    print(\"  Troubleshooting suggestions:\")\n",
    "    print(\"    - Ensure you are logged into Hugging Face (`notebook_login()`).\")\n",
    "    print(\"    - Double-check the model name for typos.\")\n",
    "    print(\"    - Ensure your Colab environment has a compatible GPU selected and enough resources.\")\n",
    "    Settings.llm = None # Ensure LLM is None if setup fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Assemble the RAG Query Engine\n",
    "\n",
    "---\n",
    "This code snippet assembles the core components of our Retrieval Augmented Generation (RAG) system.\n",
    "\n",
    "1.  **Retriever**: It creates a retriever from our previously loaded vector index (`index`), configured to fetch the top 7 most relevant document chunks (`similarity_top_k=7`) for any query.\n",
    "2.  **Prompt Template**: It uses the `QA_PROMPT_TEMPLATE` constant defined earlier, telling the LLM how to behave as a research assistant, emphasizing using *only* the provided CORD-19 context.\n",
    "3.  **Response Synthesizer**: This component takes the retrieved chunks, the user's query, and the prompt template, and uses the configured LLM to generate the final textual answer. We enable `streaming=True` here for a better user experience later in the UI.\n",
    "4.  **Query Engine**: Finally, it combines the retriever and response synthesizer into a `RetrieverQueryEngine`, which is our complete system for answering questions based on the CORD-19 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, PromptTemplate\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "print(\"--- Creating the Base RAG Query Engine ---\")\n",
    "\n",
    "# Initialize the engine variable\n",
    "base_query_engine = None\n",
    "\n",
    "# Ensure the index is loaded and Settings.llm is configured from previous steps\n",
    "if 'index' in locals() and index is not None and Settings.llm is not None:\n",
    "\n",
    "    # --- 1. Create the Retriever ---\n",
    "    print(\"Creating retriever from the index...\")\n",
    "    # Retrieve the top 7 most similar chunks for a balance of speed and context (value updated from 5 to 7).\n",
    "    retriever = index.as_retriever(similarity_top_k=7)\n",
    "    print(f\"✅ Retriever configured to fetch top {retriever.similarity_top_k} chunks.\")\n",
    "\n",
    "    # --- 2. Use the Custom QA Prompt Template ---\n",
    "    print(\"\\nUsing global QA prompt template...\")\n",
    "    if 'QA_PROMPT_TEMPLATE' not in globals():\n",
    "        print(\"❌ QA_PROMPT_TEMPLATE not found. Please run 'Define Global Constants' cell.\")\n",
    "        # Fallback to default if constant is missing\n",
    "        qa_prompt_template = PromptTemplate(\"Context: {context_str}\\n\\nQuery: {query_str}\\n\\nAnswer:\")\n",
    "    else:\n",
    "        qa_prompt_template = QA_PROMPT_TEMPLATE\n",
    "    print(\"✅ QA prompt template ready.\")\n",
    "\n",
    "    # --- 3. Configure the Response Synthesizer and Assemble the Query Engine ---\n",
    "    print(\"\\nAssembling the Base RAG Query Engine...\")\n",
    "    # This component takes the retrieved chunks and the prompt, and uses the LLM to generate the answer.\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "        response_mode=\"compact\", # Efficient mode for synthesizing responses\n",
    "        text_qa_template=qa_prompt_template, # Our custom prompt\n",
    "        llm=Settings.llm, # The configured LLM\n",
    "        streaming=True # Enable streaming for faster perceived response in UI later\n",
    "    )\n",
    "\n",
    "    # Assemble the final query engine using the retriever and response synthesizer.\n",
    "    base_query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "    )\n",
    "    print(\"\\n✅ Base RAG Query Engine assembled successfully.\")\n",
    "    print(f\"  It will use the {LLM_MODEL_NAME} and retrieve {retriever.similarity_top_k} context chunks.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n❌ Index object or LLM (Settings.llm) is not available.\")\n",
    "    if 'index' not in locals() or index is None:\n",
    "        print(\"  - Index is missing. Please ensure it was loaded or created correctly.\")\n",
    "    if Settings.llm is None:\n",
    "        print(\"  - LLM is missing. Please ensure it was configured correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Create the Conversational Chat Engine\n",
    "\n",
    "---\n",
    "This code snippet builds upon our `base_query_engine` to create a more advanced `CondenseQuestionChatEngine`.\n",
    "This new engine is designed for conversational interactions.\n",
    "\n",
    "**Condense Question Logic:** When you ask a follow-up question (e.g., \"what else?\" or just \"smoking\"), this engine first looks at the chat history and your new input.\n",
    "It then uses the LLM (with our `CONDENSE_PROMPT_TEMPLATE`) to rewrite your input into a complete, standalone question that makes sense given the conversation so far.\n",
    "\n",
    "**Uses Base Engine:** This newly formulated standalone question is then passed to our `base_query_engine` (which is excellent at answering specific, well-formed questions using the CORD-19 data).\n",
    "This approach allows the agent to handle vague follow-ups and maintain conversational context effectively.\n",
    "The `verbose=False` setting for now means it won't print the condensed questions during operation, keeping the output clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "\n",
    "print(\"--- Creating the Conversational Chat Engine ---\")\n",
    "\n",
    "# Initialize the conversational chat engine variable\n",
    "conversational_chat_engine = None\n",
    "\n",
    "if 'base_query_engine' in locals() and base_query_engine is not None and Settings.llm is not None:\n",
    "\n",
    "    try:\n",
    "        # Use the global CONDENSE_PROMPT_TEMPLATE\n",
    "        if 'CONDENSE_PROMPT_TEMPLATE' not in globals():\n",
    "            print(\"❌ CONDENSE_PROMPT_TEMPLATE not found. Please run 'Define Global Constants' cell.\")\n",
    "            # Fallback to default if constant is missing\n",
    "            condense_template = PromptTemplate(\"Chat History: {chat_history}\\nQuestion: {question}\\nStandalone Question:\")\n",
    "        else:\n",
    "            condense_template = CONDENSE_PROMPT_TEMPLATE\n",
    "\n",
    "        # Create the CondenseQuestionChatEngine.\n",
    "        conversational_chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "            query_engine=base_query_engine, # The engine we built in 'Assemble the RAG Query Engine'\n",
    "            condense_template=condense_template, # Our custom condense prompt\n",
    "            verbose=False # Keep verbose=False here to not spam output for this cell\n",
    "        )\n",
    "        print(\"\\n✅ Conversational Chat Engine (CondenseQuestionChatEngine) created successfully.\")\n",
    "        print(\"  It will use the base RAG query engine to answer questions after rephrasing them based on chat history.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ CRITICAL ERROR: Failed to create CondenseQuestionChatEngine. Error: {e}\")\n",
    "        print(\"  Ensure 'base_query_engine' was created successfully in the previous step.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n❌ Base Query Engine ('base_query_engine') or LLM (Settings.llm) is not available.\")\n",
    "    if 'base_query_engine' not in locals() or base_query_engine is None:\n",
    "        print(\"  - 'base_query_engine' is missing. Please ensure 'Assemble the RAG Query Engine' was completed successfully.\")\n",
    "    if Settings.llm is None:\n",
    "        print(\"  - LLM is missing. Please ensure 'Configure the Large Language Model (LLM)' was successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Application Development (Gradio GUI)\n",
    "\n",
    "## 8.1 Structure Code for UI Application\n",
    "\n",
    "---\n",
    "\n",
    "This code defines a single, crucial function called `initialize_ai_system()`.\n",
    "This function encapsulates all the setup steps required to get our AI agent ready:\n",
    "\n",
    "1.  Configuring the embedding model.\n",
    "2.  Loading our saved vector index.\n",
    "3.  Configuring the LLM.\n",
    "4.  Assembling the complete RAG query engine (retriever, prompt template, response synthesizer).\n",
    "5.  Creating the conversational chat engine.\n",
    "\n",
    "It also includes a simple caching mechanism (`CACHED_CHAT_ENGINE`) so that this entire setup process only runs once per Colab session,\n",
    "making subsequent uses of the agent much faster. This function is essential for a clean and efficient Gradio application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch # For torch.float16\n",
    "import gradio as gr\n",
    "\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    PromptTemplate\n",
    ")\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# --- Global Cache for the Chat Engine ---\n",
    "CACHED_CHAT_ENGINE = None\n",
    "\n",
    "def initialize_ai_system():\n",
    "    \"\"\"\n",
    "    Initializes all AI components (embedding model, index, LLM,\n",
    "    base query engine, and then the CondenseQuestionChatEngine with a custom condense prompt).\n",
    "    Caches and returns the CondenseQuestionChatEngine.\n",
    "    \"\"\"\n",
    "    global CACHED_CHAT_ENGINE\n",
    "\n",
    "    if CACHED_CHAT_ENGINE is not None:\n",
    "        print(\"Returning cached AI system (CondenseQuestionChatEngine).\")\n",
    "        return CACHED_CHAT_ENGINE\n",
    "\n",
    "    print(\"--- Initializing AI System with CondenseQuestionChatEngine (Custom Condense Prompt) ---\")\n",
    "\n",
    "    # Ensure global constants are available (run cell 7.1 if not)\n",
    "    if 'EMBEDDING_MODEL_NAME' not in globals():\n",
    "        print(\"❌ Global constants (EMBEDDING_MODEL_NAME, LLM_MODEL_NAME, etc.) not defined. Please run cell 7.1 first.\")\n",
    "        # Attempt to define minimal constants for fallback\n",
    "        global EMBEDDING_MODEL_NAME, LLM_MODEL_NAME, LLM_CONTEXT_WINDOW, LLM_MAX_NEW_TOKENS, LLM_TEMPERATURE, QA_PROMPT_TEMPLATE, CONDENSE_PROMPT_TEMPLATE\n",
    "        EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        LLM_MODEL_NAME = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "        LLM_CONTEXT_WINDOW, LLM_MAX_NEW_TOKENS, LLM_TEMPERATURE = 2048, 256, 0.7\n",
    "        QA_PROMPT_TEMPLATE = PromptTemplate(\"Context: {context_str}\\n\\nQuery: {query_str}\\n\\nAnswer:\")\n",
    "        CONDENSE_PROMPT_TEMPLATE = PromptTemplate(\"Chat History: {chat_history}\\nQuestion: {question}\\nStandalone Question:\")\n",
    "        print(\"  Attempted to set minimal constants. Please run cell 7.1 for full functionality.\")\n",
    "\n",
    "    # --- 1. CONFIGURE EMBEDDING MODEL ---\n",
    "    print(\"Configuring embedding model...\")\n",
    "    try:\n",
    "        Settings.embed_model = HuggingFaceEmbedding(\n",
    "            model_name=EMBEDDING_MODEL_NAME,\n",
    "            device=\"cuda\"\n",
    "        )\n",
    "        print(\"✅ Embedding model configured.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR configuring embedding model: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 2. LOAD THE VECTOR INDEX ---\n",
    "    print(\"\\nLoading vector index...\")\n",
    "    persist_dir = \"\"\n",
    "    drive_path = \"/content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\"\n",
    "    colab_local_path = \"storage_cord19_smoking_index_colab\"\n",
    "\n",
    "    if os.path.exists(drive_path):\n",
    "        persist_dir = drive_path\n",
    "        print(f\"  Attempting to load index from Google Drive: {persist_dir}\")\n",
    "    elif os.path.exists(colab_local_path):\n",
    "        persist_dir = colab_local_path\n",
    "        print(f\"  Attempting to load index from local Colab storage: {persist_dir}\")\n",
    "    else:\n",
    "        print(f\"❌ CRITICAL ERROR: Index directory not found at expected Google Drive path ('{drive_path}') or local Colab path ('{colab_local_path}').\")\n",
    "        return None\n",
    "\n",
    "    if not os.path.exists(persist_dir) or not os.listdir(persist_dir):\n",
    "        print(f\"❌ CRITICAL ERROR: Selected persist_dir ('{persist_dir}') does not exist or is empty. Cannot load index.\")\n",
    "        print(\"  Please ensure you have run 'Create and Persist VectorStoreIndex' successfully to save the index.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "        index = load_index_from_storage(storage_context)\n",
    "        print(\"✅ Vector index loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ CRITICAL ERROR: Failed to load index from {persist_dir}. Error: {e}\")\n",
    "        print(\"  The index might be corrupted or incompatible. Consider deleting the directory and regenerating it.\")\n",
    "        return None\n",
    "\n",
    "    # --- 3. CONFIGURE LLM ---\n",
    "    print(\"\\nConfiguring LLM...\")\n",
    "    try:\n",
    "        llm = HuggingFaceLLM(\n",
    "            model_name=LLM_MODEL_NAME,\n",
    "            tokenizer_name=LLM_MODEL_NAME,\n",
    "            context_window=LLM_CONTEXT_WINDOW,\n",
    "            max_new_tokens=LLM_MAX_NEW_TOKENS,\n",
    "            device_map=\"auto\",\n",
    "            model_kwargs={\"torch_dtype\": torch.float16},\n",
    "            generate_kwargs={\"temperature\": LLM_TEMPERATURE, \"do_sample\": True}\n",
    "        )\n",
    "        Settings.llm = llm\n",
    "        print(f\"✅ LLM ({LLM_MODEL_NAME}) configured successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ CRITICAL ERROR: Failed to configure LLM. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 4. ASSEMBLE BASE RAG QUERY ENGINE ---\n",
    "    print(\"\\nAssembling Base RAG Query Engine...\")\n",
    "    try:\n",
    "        retriever = index.as_retriever(similarity_top_k=7) # Maintain 7 here\n",
    "\n",
    "        response_synthesizer = get_response_synthesizer(\n",
    "            response_mode=\"compact\",\n",
    "            text_qa_template=QA_PROMPT_TEMPLATE,\n",
    "            llm=Settings.llm,\n",
    "            streaming=True\n",
    "        )\n",
    "\n",
    "        base_query_engine = RetrieverQueryEngine(\n",
    "            retriever=retriever,\n",
    "            response_synthesizer=response_synthesizer,\n",
    "        )\n",
    "        print(\"✅ Base RAG Query Engine assembled successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ CRITICAL ERROR: Failed to assemble Base RAG Query Engine. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 5. CREATE CONDENSE QUESTION CHAT ENGINE ---\n",
    "    print(\"\\nCreating CondenseQuestionChatEngine with custom prompt...\")\n",
    "    try:\n",
    "        condense_chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "            query_engine=base_query_engine,\n",
    "            condense_template=CONDENSE_PROMPT_TEMPLATE, # Use the global custom prompt\n",
    "            verbose=True # Set verbose=True for debugging condensed questions\n",
    "        )\n",
    "        print(\"✅ CondenseQuestionChatEngine created successfully with custom condense prompt.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ CRITICAL ERROR: Failed to create CondenseQuestionChatEngine. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 6. CACHE AND RETURN THE CHAT ENGINE ---\n",
    "    print(\"\\nCaching the CondenseQuestionChatEngine.\")\n",
    "    CACHED_CHAT_ENGINE = condense_chat_engine\n",
    "    print(\"\\n✅ AI System Initialized with smarter CondenseQuestionChatEngine and Cached Successfully.\")\n",
    "    return CACHED_CHAT_ENGINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Develop the Gradio Application\n",
    "\n",
    "---\n",
    "This code snippet builds and launches your interactive web UI using Gradio.\n",
    "-   The `chat_response_streaming(message, history)` function handles user input, first checking for simple greetings/affirmations for a quick, direct response, then passing complex queries to the AI engine.\n",
    "    It uses `yield` to stream the AI's response for a responsive user experience.\n",
    "-   **Pre-initialization**: We explicitly call `initialize_ai_system()` before launching the UI.\n",
    "    This ensures that the potentially slow setup (model loading, etc.) happens once before the UI link is generated, preventing timeouts or slow first interactions.\n",
    "-   `gr.ChatInterface`: This Gradio component quickly creates a full chatbot UI, configured to use our `chat_response_streaming` function.\n",
    "-   `iface.launch(share=True, debug=True)`: This launches the web server. `share=True` generates a public URL (valid for ~72 hours).\n",
    "    `debug=True` shows any Gradio-specific errors in the Colab output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "# `initialize_ai_system` is defined in the previous cell.\n",
    "\n",
    "def chat_response_streaming(message, history):\n",
    "    \"\"\"\n",
    "    Handles a user's message, pre-filters for some casual inputs,\n",
    "    gets a response from the AI system for others, and streams it back.\n",
    "    \"\"\"\n",
    "    print(f\"\\nUser query for Gradio: '{message}'\")\n",
    "    normalized_message = message.strip().lower()\n",
    "\n",
    "    # --- Simple Pre-filter for Common Casual Inputs (Fast Direct Responses) ---\n",
    "    if normalized_message in [\"hi\", \"hello\", \"hey\", \"greetings\"]:\n",
    "        yield \"Hello there! I'm an AI assistant focused on COVID-19 and smoking. How can I help with your research today?\"\n",
    "        return\n",
    "\n",
    "    if normalized_message.startswith(\"my name is\"):\n",
    "        try:\n",
    "            name_part = message.split(\"my name is\", 1)[1].strip()\n",
    "            if name_part:\n",
    "                name = name_part.split(\" \")[0].capitalize()\n",
    "                yield f\"Nice to meet you, {name}! I can assist with questions about COVID-19 and smoking. What's your query?\"\n",
    "            else:\n",
    "                yield \"Okay! I'm here to help with your research on COVID-19 and smoking.\"\n",
    "            return\n",
    "        except IndexError:\n",
    "            yield \"Okay! I'm here to help with your research on COVID-19 and smoking.\"\n",
    "            return\n",
    "\n",
    "    # Handle common affirmations/closings\n",
    "    common_affirmations = [\"perfect\", \"great\", \"thanks\", \"thank you\", \"ok\", \"okay\", \"got it\", \"sounds good\", \"excellent\"]\n",
    "    if normalized_message in common_affirmations:\n",
    "        yield \"You're welcome! Is there anything else I can help you with regarding COVID-19 and smoking research?\"\n",
    "        return\n",
    "\n",
    "    # --- End of Pre-filter ---\n",
    "\n",
    "    # If not caught by pre-filters, proceed with the AI engine\n",
    "    chat_engine_instance = initialize_ai_system()\n",
    "\n",
    "    if not chat_engine_instance:\n",
    "        yield \"Error: The AI chat engine is not available. Please check the Colab notebook for errors during initialization.\"\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # The `stream_chat` method expects a list of chat_history tuples (user_message, assistant_response)\n",
    "        # Gradio's history is already in this format.\n",
    "        # We need to convert it to LlamaIndex's ChatMessage format if directly passing history.\n",
    "        # However, CondenseQuestionChatEngine automatically manages history for its `stream_chat` method.\n",
    "        # The 'history' argument in gradio's function is the full conversation history. \n",
    "        # LlamaIndex's chat engine takes the 'message' as the *current* user input, and maintains its own internal chat history.\n",
    "        # Thus, we just pass the current message.\n",
    "        response_stream = chat_engine_instance.stream_chat(message)\n",
    "        accumulated_response = \"\"\n",
    "        for token in response_stream.response_gen:\n",
    "            accumulated_response += token\n",
    "            yield accumulated_response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gradio query engine processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        yield f\"Sorry, an error occurred while processing your request: {str(e)}\"\n",
    "\n",
    "# --- Pre-initialize the AI system before launching the UI ---\n",
    "print(\"Pre-initializing AI system for Gradio Interface... This might take a few minutes if it's the first run in this session.\")\n",
    "engine_instance = initialize_ai_system()\n",
    "\n",
    "if engine_instance is None:\n",
    "    print(\"CRITICAL ERROR: Could not initialize AI system for Gradio. The UI cannot be launched reliably. Please review logs above.\")\n",
    "else:\n",
    "    print(\"AI system pre-initialized successfully and is cached.\")\n",
    "\n",
    "    title = \"COVID-19 & Smoking Research Assistant\"\n",
    "\n",
    "    iface = gr.ChatInterface(\n",
    "        fn=chat_response_streaming,\n",
    "        title=title,\n",
    "        description=\"Ask questions about the relationship between COVID-19 and smoking, based on the CORD-19 dataset. Powered by a Llama 3 8B model.\",\n",
    "        examples=[\n",
    "            [\"What is the link between smoking and COVID-19 severity?\"],\n",
    "            [\"Does vaping affect COVID-19 outcomes?\"],\n",
    "            [\"Are smokers more susceptible to COVID-19?\"],\n",
    "            [\"What is nicotine's role?\"],\n",
    "            [\"Tell me more about the impact of tobacco.\"]\n",
    "        ],\n",
    "        chatbot=gr.Chatbot(height=600, label=\"Chat Conversation\"),\n",
    "        textbox=gr.Textbox(placeholder=\"Type your question here and press Enter...\", container=False, scale=7, label=\"Your Question\")\n",
    "    )\n",
    "\n",
    "    print(\"\\nLaunching Gradio Interface... Please wait for the public URL.\")\n",
    "    iface.launch(share=True, debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
```
