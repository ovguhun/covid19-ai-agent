{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhCi00Odz0OK"
      },
      "source": [
        "# COVID-19 MedQuad Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK5lCEx1z4ZE"
      },
      "source": [
        "# Creating an LLM-based AI Research Assistant for the CORD-19 Dataset.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "'The CORD-19 (COVID-19 Open Research Dataset) is a comprehensive collection of\n",
        "scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses.\n",
        "This project aims to build an AI agent that specializes in analyzing this dataset to answer questions about the relationship between COVID-19 and smoking (including cigarettes, vaping, and tobacco). The agent will leverage a Large Language Model (LLM) and Retrieval Augmented Generation (RAG) to provide insights based on the scientific literature within CORD-19."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477RASRi0vOU"
      },
      "source": [
        "# Install All Necessary Libraries\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The !pip install commands will then install all the required Python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "faCD_R-cuW3i",
        "outputId": "897d23fa-d773-428e-887f-5df9d821e224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.11)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (0.12.37)\n",
            "Requirement already satisfied: llama-index-embeddings-huggingface in /usr/local/lib/python3.11/dist-packages (0.5.4)\n",
            "Requirement already satisfied: llama-index-llms-huggingface in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.8)\n",
            "Requirement already satisfied: llama-index-cli<0.5,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.36 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.37)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.6.11)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.44)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.7)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.31.2)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-huggingface) (4.1.0)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-huggingface) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.13.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.11.15)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.82.0)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (2.1.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (1.2.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (2.11.4)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.36->llama-index) (2.0.40)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.22)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (5.5.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.23)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.15.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.5.3)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.20.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index) (1.7.3)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.7)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.4.26)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.36->llama-index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.36->llama-index) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.36->llama-index) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.36->llama-index) (0.16.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.23 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.23)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.36->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.36->llama-index) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.36->llama-index) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.36->llama-index) (3.2.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.36->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.36->llama-index) (3.26.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.23->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index) (0.4.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to install all required libraries\n",
        "!pip install gradio\n",
        "!pip install llama-index llama-index-embeddings-huggingface llama-index-llms-huggingface\n",
        "!pip install pandas pyarrow tqdm\n",
        "!pip install transformers accelerate bitsandbytes torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2srLzzde1nuu"
      },
      "source": [
        "# Verify that PyTorch can see and use the CUDA-enabled GPU.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "F8RbYfGvvNX_",
        "outputId": "ea286fd1-22b5-4b2b-bb5a-4ca621393aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "CUDA version: 12.4\n",
            "GPU name: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\") # 0 refers to the first GPU\n",
        "else:\n",
        "    print(\"WARNING: CUDA not available. Check Colab runtime settings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkJTpYBI2ErT"
      },
      "source": [
        "#Hugging Face Hub Setup\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code snippet uses the notebook_login function from the huggingface_hub library to securely connect your Colab notebook to your Hugging Face account. This authentication is necessary to download the dataset and the language model we'll be using later. You'll be prompted to enter an access token from your Hugging Face account settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fKtVqP26ve_U",
        "outputId": "7d2b9108-395b-4f4c-bd75-bd683bd47e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "The token `Agent` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Agent`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qt9IMBl2eoe"
      },
      "source": [
        "# Google Drive Integration\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code mounts your Google Drive to the Colab environment. This allows you to save important files, like the vector index we will create later, directly to your personal Google Drive. This prevents data loss if your Colab session disconnects and saves you from having to rebuild the index every time you open the notebook. You'll be asked to authorize Colab to access your Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK0JOzOrxM-r",
        "outputId": "1f54704f-6d2e-4db5-a03a-8ee627948ca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaVZM49h3_MI"
      },
      "source": [
        "#Download and Load the Dataset\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXjou97iyL9z",
        "outputId": "63dbbc8e-1781-4f40-ff9d-63be00d69e27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load the CORD-19 abstracts dataset from Hugging Face...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Successfully loaded dataset.\n",
            "The dataset has 368618 abstracts.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(\"Attempting to load the CORD-19 abstracts dataset from Hugging Face...\")\n",
        "\n",
        "try:\n",
        "    # This reads the dataset directly into a pandas DataFrame.\n",
        "    # It might take a moment to download.\n",
        "    df_cord19_abstracts = pd.read_parquet(\"hf://datasets/pritamdeka/cord-19-abstract/data/train-00000-of-00001.parquet\")\n",
        "    print(\"\\n✅ Successfully loaded dataset.\")\n",
        "    print(f\"The dataset has {len(df_cord19_abstracts)} abstracts.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error loading dataset from Hugging Face: {e}\")\n",
        "    print(\"Please ensure you are logged in with 'notebook_login()' and have network access.\")\n",
        "    df_cord19_abstracts = pd.DataFrame() # Create empty df to avoid later errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeyIz-hv4JSw"
      },
      "source": [
        "# Data Acquisition & Preprocessing\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Inspect and Clean Data**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code inspects the loaded data to understand its structure. .info() provides a technical summary (columns, data types), and .head() shows the first 5 rows to give us a look at the actual content. We then perform basic cleaning by removing any rows that might have an empty abstract and ensuring the abstract column is treated as text, which prevents errors in later steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "UVt07U-Szf3D",
        "outputId": "b85b380d-c315-4a15-d03b-58be7e441b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dataset Information ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 368618 entries, 0 to 368617\n",
            "Data columns (total 1 columns):\n",
            " #   Column    Non-Null Count   Dtype \n",
            "---  ------    --------------   ----- \n",
            " 0   abstract  368618 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 2.8+ MB\n",
            "\n",
            "\n",
            "--- First 5 Rows of the Dataset ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                            abstract\n",
              "0  OBJECTIVE: This retrospective chart review des...\n",
              "1  Inflammatory diseases of the respiratory tract...\n",
              "2  Surfactant protein-D (SP-D) participates in th...\n",
              "3  Endothelin-1 (ET-1) is a 21 amino acid peptide...\n",
              "4  Respiratory syncytial virus (RSV) and pneumoni..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-175c6ee0-2bf7-47d5-9722-1f5f86fe48a0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OBJECTIVE: This retrospective chart review des...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Inflammatory diseases of the respiratory tract...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Surfactant protein-D (SP-D) participates in th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Endothelin-1 (ET-1) is a 21 amino acid peptide...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Respiratory syncytial virus (RSV) and pneumoni...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-175c6ee0-2bf7-47d5-9722-1f5f86fe48a0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-175c6ee0-2bf7-47d5-9722-1f5f86fe48a0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-175c6ee0-2bf7-47d5-9722-1f5f86fe48a0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-59704989-2ca7-4b43-b153-dd2cf5a971ce\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-59704989-2ca7-4b43-b153-dd2cf5a971ce')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-59704989-2ca7-4b43-b153-dd2cf5a971ce button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"\\u274c DataFrame 'df_cord19_abstracts' was not loaded correctly in the previous step\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Inflammatory diseases of the respiratory tract are commonly associated with elevated production of nitric oxide (NO\\u2022) and increased indices of NO\\u2022 -dependent oxidative stress. Although NO\\u2022 is known to have anti-microbial, anti-inflammatory and anti-oxidant properties, various lines of evidence support the contribution of NO\\u2022 to lung injury in several disease models. On the basis of biochemical evidence, it is often presumed that such NO\\u2022 -dependent oxidations are due to the formation of the oxidant peroxynitrite, although alternative mechanisms involving the phagocyte-derived heme proteins myeloperoxidase and eosinophil peroxidase might be operative during conditions of inflammation. Because of the overwhelming literature on NO\\u2022 generation and activities in the respiratory tract, it would be beyond the scope of this commentary to review this area comprehensively. Instead, it focuses on recent evidence and concepts of the presumed contribution of NO\\u2022 to inflammatory diseases of the lung.\",\n          \"Respiratory syncytial virus (RSV) and pneumonia virus of mice (PVM) are viruses of the family Paramyxoviridae, subfamily pneumovirus, which cause clinically important respiratory infections in humans and rodents, respectively. The respiratory epithelial target cells respond to viral infection with specific alterations in gene expression, including production of chemoattractant cytokines, adhesion molecules, elements that are related to the apoptosis response, and others that remain incompletely understood. Here we review our current understanding of these mucosal responses and discuss several genomic approaches, including differential display reverse transcription-polymerase chain reaction (PCR) and gene array strategies, that will permit us to unravel the nature of these responses in a more complete and systematic manner.\",\n          \"Surfactant protein-D (SP-D) participates in the innate response to inhaled microorganisms and organic antigens, and contributes to immune and inflammatory regulation within the lung. SP-D is synthesized and secreted by alveolar and bronchiolar epithelial cells, but is also expressed by epithelial cells lining various exocrine ducts and the mucosa of the gastrointestinal and genitourinary tracts. SP-D, a collagenous calcium-dependent lectin (or collectin), binds to surface glycoconjugates expressed by a wide variety of microorganisms, and to oligosaccharides associated with the surface of various complex organic antigens. SP-D also specifically interacts with glycoconjugates and other molecules expressed on the surface of macrophages, neutrophils, and lymphocytes. In addition, SP-D binds to specific surfactant-associated lipids and can influence the organization of lipid mixtures containing phosphatidylinositol in vitro. Consistent with these diverse in vitro activities is the observation that SP-D-deficient transgenic mice show abnormal accumulations of surfactant lipids, and respond abnormally to challenge with respiratory viruses and bacterial lipopolysaccharides. The phenotype of macrophages isolated from the lungs of SP-D-deficient mice is altered, and there is circumstantial evidence that abnormal oxidant metabolism and/or increased metalloproteinase expression contributes to the development of emphysema. The expression of SP-D is increased in response to many forms of lung injury, and deficient accumulation of appropriately oligomerized SP-D might contribute to the pathogenesis of a variety of human lung diseases.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- Cleaning Data ---\n",
            "✅ Data cleaned: Removed empty abstracts and ensured text format.\n",
            "The dataset now has 368618 abstracts after cleaning.\n"
          ]
        }
      ],
      "source": [
        "# Inspect the structure of the loaded data and perform basic cleaning.\n",
        "\n",
        "# Check if the DataFrame from the previous step exists\n",
        "if 'df_cord19_abstracts' in locals() and not df_cord19_abstracts.empty:\n",
        "    print(\"--- Dataset Information ---\")\n",
        "    df_cord19_abstracts.info()\n",
        "\n",
        "    print(\"\\n\\n--- First 5 Rows of the Dataset ---\")\n",
        "    # Using display() in Colab provides a nicer table format\n",
        "    display(df_cord19_abstracts.head())\n",
        "\n",
        "    # --- Data Cleaning ---\n",
        "    print(\"\\n\\n--- Cleaning Data ---\")\n",
        "    # Remove rows where the 'abstract' column is empty or missing\n",
        "    df_cord19_abstracts.dropna(subset=['abstract'], inplace=True)\n",
        "    # Ensure the abstract column is treated as the 'string' data type\n",
        "    df_cord19_abstracts['abstract'] = df_cord19_abstracts['abstract'].astype(str)\n",
        "\n",
        "    print(\"✅ Data cleaned: Removed empty abstracts and ensured text format.\")\n",
        "    print(f\"The dataset now has {len(df_cord19_abstracts)} abstracts after cleaning.\")\n",
        "else:\n",
        "    print(\"❌ DataFrame 'df_cord19_abstracts' was not loaded correctly in the previous step. Please check if you Load the Dataset successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYUnUZcw5TUN"
      },
      "source": [
        "**Keyword-based Filtering**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code snippet filters our large DataFrame of abstracts to create a smaller, more focused one. First, it defines a keywords list containing terms related to smoking. It then uses pandas' powerful str.contains() function to search the 'abstract' column in a case-insensitive way for any of these keywords. The result is a new DataFrame, df_relevant_abstracts, containing only the documents relevant to our research question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "XXhqZayk0EAm",
        "outputId": "689241d4-a5e3-4115-bf2b-e5920daad9b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Filtering Abstracts by Keywords ---\n",
            "Keywords for filtering: ['smoking', 'cigarette', 'nicotine', 'vaping', 'tobacco', 'e-cigarette', 'smoker']\n",
            "\n",
            "✅ Found 2677 relevant abstracts after keyword filtering.\n",
            "\n",
            "--- First 5 Relevant Abstracts ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                              abstract\n",
              "8    Heme oxygenase-1 (HO-1), an inducible stress p...\n",
              "41   BACKGROUND AND METHODS: Human metapneumovirus ...\n",
              "43   BACKGROUND: The present study aimed to provide...\n",
              "472  To date, at least 900 different microRNA (miRN...\n",
              "506  Genetic material in plants is distributed into..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4921ef8-2efe-4e90-87b8-2327feb37729\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Heme oxygenase-1 (HO-1), an inducible stress p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>BACKGROUND AND METHODS: Human metapneumovirus ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>BACKGROUND: The present study aimed to provide...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472</th>\n",
              "      <td>To date, at least 900 different microRNA (miRN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>Genetic material in plants is distributed into...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4921ef8-2efe-4e90-87b8-2327feb37729')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a4921ef8-2efe-4e90-87b8-2327feb37729 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a4921ef8-2efe-4e90-87b8-2327feb37729');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b990d874-9c01-4b57-9262-e9a2cd1ad794\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b990d874-9c01-4b57-9262-e9a2cd1ad794')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b990d874-9c01-4b57-9262-e9a2cd1ad794 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    df_relevant_abstracts = pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"BACKGROUND AND METHODS: Human metapneumovirus (hMPV) is a recently discovered respiratory virus associated with bronchiolitis, pneumonia, croup and exacerbations of asthma. Since respiratory viruses are frequently detected in patients with acute exacerbations of COPD (AE-COPD) it was our aim to investigate the frequency of hMPV detection in a prospective cohort of hospitalized patients with AE-COPD compared to patients with stable COPD and to smokers without by means of quantitative real-time RT-PCR. RESULTS: We analysed nasal lavage and induced sputum of 130 patients with AE-COPD, 65 patients with stable COPD and 34 smokers without COPD. HMPV was detected in 3/130 (2.3%) AE-COPD patients with a mean of 6.5 \\u00d7 10(5 )viral copies/ml in nasal lavage and 1.88 \\u00d7 10(5 )viral copies/ml in induced sputum. It was not found in patients with stable COPD or smokers without COPD. CONCLUSION: HMPV is only found in a very small number of patients with AE-COPD. However it should be considered as a further possible viral trigger of AE-COPD because asymptomatic carriage is unlikely.\",\n          \"Genetic material in plants is distributed into nucleus, plastids and mitochondria. Plastid has a central role of carrying out photosynthesis in plant cells. Plastid transformation is becoming more popular and an alternative to nuclear gene transformation because of various advantages like high protein levels, the feasibility of expressing multiple proteins from polycistronic mRNAs, and gene containment through the lack of pollen transmission. Recently, much progress in plastid engineering has been made. In addition to model plant tobacco, many transplastomic crop plants have been generated which possess higher resistance to biotic and abiotic stresses and molecular pharming. In this mini review, we will discuss the features of the plastid DNA and advantages of plastid transformation. We will also present some examples of transplastomic plants developed so far through plastid engineering, and the various applications of plastid transformation.\",\n          \"BACKGROUND: The present study aimed to provide information on awareness of the attributable fraction of cancer causes among the Japanese general population. METHODS: A nationwide representative sample of 2,000 Japanese aged 20 or older was asked about their perception and level of concern about various environmental and genetic risk factors in relation to cancer prevention, as a part of an Omnibus Survey. Interviews were conducted with 1,355 subjects (609 men and 746 women). RESULTS: Among 12 risk factor candidates, the attributable fraction of cancer-causing viral and bacterial infection was considered highest (51%), followed by that of tobacco smoking (43%), stress (39%), and endocrine-disrupting chemicals (37%). On the other hand, the attributable fractions of cancer by charred fish and meat (21%) and alcohol drinking (22%) were considered low compared with other risk factor candidates. For most risk factors, attributable fraction responses were higher in women than in men. As a whole, the subjects tended to respond with higher values than those estimated by epidemiologic evidence in the West. The attributable fraction of cancer speculated to be genetically determined was 32%, while 36% of cancer was considered preventable by improving lifestyle. CONCLUSION: Our results suggest that awareness of the attributable fraction of cancer causes in the Japanese general population tends to be dominated by cancer-causing infection, occupational exposure, air pollution and food additives rather than major lifestyle factors such as diet.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Filter the DataFrame to include only abstracts containing specific keywords.\n",
        "\n",
        "# First, check if the main DataFrame from the previous steps exists\n",
        "if 'df_cord19_abstracts' in locals() and not df_cord19_abstracts.empty:\n",
        "    print(\"--- Filtering Abstracts by Keywords ---\")\n",
        "\n",
        "    # Define your keywords related to smoking\n",
        "    keywords = [\"smoking\", \"cigarette\", \"nicotine\", \"vaping\", \"tobacco\", \"e-cigarette\", \"smoker\"]\n",
        "    print(f\"Keywords for filtering: {keywords}\")\n",
        "\n",
        "    # Create a search pattern: \"keyword1|keyword2|keyword3\" which means \"keyword1 OR keyword2 OR keyword3\"\n",
        "    search_terms_pattern = '|'.join(keywords)\n",
        "\n",
        "    # Filter the DataFrame and create a new one with only relevant abstracts\n",
        "    df_relevant_abstracts = df_cord19_abstracts[df_cord19_abstracts['abstract'].str.contains(search_terms_pattern, case=False, na=False)]\n",
        "\n",
        "    print(f\"\\n✅ Found {len(df_relevant_abstracts)} relevant abstracts after keyword filtering.\")\n",
        "\n",
        "    # Display the first few relevant abstracts to confirm the filtering worked\n",
        "    if len(df_relevant_abstracts) > 0:\n",
        "        print(\"\\n--- First 5 Relevant Abstracts ---\")\n",
        "        display(df_relevant_abstracts.head())\n",
        "    else:\n",
        "        print(\"\\n⚠️ Warning: No relevant abstracts were found for the given keywords.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ DataFrame 'df_cord19_abstracts' was not found or is empty. Please run the previous steps first.\")\n",
        "    # Create an empty DataFrame to prevent errors in later cells\n",
        "    df_relevant_abstracts = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTb_jJ_T8CKd"
      },
      "source": [
        "# Vector Database Creation & Management\n",
        "\n",
        "**Configure LlamaIndex Settings**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code snippet configures LlamaIndex's global settings. It specifies which model to use for creating vector embeddings (sentence-transformers/all-MiniLM-L6-v2) and explicitly tells LlamaIndex to use the GPU (cuda) for this process, which significantly speeds it up. The Settings.llm is set to None for now, as we only need the embedding model in this phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1buq6HCw2Wjs",
        "outputId": "7dfa362b-9f05-465d-e5ca-9d63377522a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Configuring LlamaIndex Settings for Embedding Model ---\n",
            "LLM is explicitly disabled. Using MockLLM.\n",
            "\n",
            "✅ Embedding model configured to run on CUDA.\n",
            "   LlamaIndex will now use 'all-MiniLM-L6-v2' for creating text embeddings.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "print(\"--- Configuring LlamaIndex Settings for Embedding Model ---\")\n",
        "\n",
        "# Set the embedding model to be used for converting text to vectors.\n",
        "# 'all-MiniLM-L6-v2' is a popular and efficient model for this.\n",
        "# 'device=\"cuda\"' ensures the GPU is used for this computationally intensive task.\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    device=\"cuda\"\n",
        ")\n",
        "\n",
        "# We are not using a Large Language Model (LLM) in this phase,\n",
        "# so we set it to None in the global settings for now.\n",
        "Settings.llm = None\n",
        "\n",
        "print(\"\\n✅ Embedding model configured to run on CUDA.\")\n",
        "print(\"   LlamaIndex will now use 'all-MiniLM-L6-v2' for creating text embeddings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmWZr9XA8nuo"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Prepare Documents and Chunking**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code takes the filtered abstracts (from df_relevant_abstracts) and breaks each one down into smaller, 150-word \"chunks.\" Each chunk is then converted into a LlamaIndex Document object. This chunking process is important because it helps the AI pinpoint very specific pieces of information within the larger abstracts when searching for answers. A progress bar (tqdm) will show the status as it processes the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNObWnNK2kYE",
        "outputId": "8d38e49b-dceb-4ffb-d512-a11c83fa5e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Preparing and chunking 2677 relevant abstracts ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Chunking abstracts: 100%|██████████| 2677/2677 [00:00<00:00, 17104.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Created 5915 document chunks from the relevant abstracts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Document\n",
        "from tqdm import tqdm # For displaying a progress bar\n",
        "\n",
        "# This list will hold all our chunked Document objects\n",
        "documents = []\n",
        "\n",
        "# First, check if the DataFrame with relevant abstracts exists and is not empty\n",
        "if 'df_relevant_abstracts' in locals() and not df_relevant_abstracts.empty:\n",
        "    print(f\"--- Preparing and chunking {len(df_relevant_abstracts)} relevant abstracts ---\")\n",
        "\n",
        "    # Get the list of abstract texts to process from the 'abstract' column\n",
        "    texts_to_process = df_relevant_abstracts['abstract'].tolist()\n",
        "\n",
        "    # Define the desired chunk size in words\n",
        "    chunk_size_by_words = 150\n",
        "\n",
        "    # Loop through each abstract, split it into words, and create chunks\n",
        "    for text in tqdm(texts_to_process, desc=\"Chunking abstracts\"):\n",
        "        words = text.split() # Split the abstract into a list of words\n",
        "        # Iterate through the words list, taking 'chunk_size_by_words' at a time\n",
        "        for i in range(0, len(words), chunk_size_by_words):\n",
        "            # Join the words in the current chunk back into a string\n",
        "            chunk_text = \" \".join(words[i:i + chunk_size_by_words]).strip()\n",
        "            # Create a LlamaIndex Document object if the chunk is not empty\n",
        "            if chunk_text:\n",
        "                documents.append(Document(text=chunk_text))\n",
        "\n",
        "    print(f\"\\n✅ Created {len(documents)} document chunks from the relevant abstracts.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ No relevant abstracts found ('df_relevant_abstracts' is missing or empty).\")\n",
        "    print(\"   Please ensure previously (Keyword-based Filtering) ran successfully and found some abstracts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNSZkBtY9JsE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Create and Persist VectorStoreIndex**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code snippet takes the 5915 document chunks we just created and uses the configured embedding model (all-MiniLM-L6-v2 on the GPU) to convert each chunk into a numerical vector. These vectors are then stored in a VectorStoreIndex, which is an optimized data structure that allows for very fast similarity searches. The code also \"persists\" (saves) this index to a specified directory (either on your Google Drive if mounted, or in Colab's temporary storage) so it can be reloaded later without rebuilding. This step will take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "b0fb2def128143e19e482f04c4d79973",
            "404cdf68f2cf47ebbacda51542a0a822",
            "544af5ee3b6940069d578c08d303dfb0",
            "13bd72516dfd484e835b28548b1472b3",
            "dbfaaa2b1ce8400d8abdd8efd1e7f541",
            "12b1d8cc0af543febb5b741ef068be96",
            "33a1558d0de141a0bff4d14fc24f03d0",
            "6eb9954eb52a481e85963737eaf0d659",
            "6c49cfb7f81b4768b32678142e536804",
            "2079fded91324d34b835829a927d7154",
            "b9e7968b463d4549893cc88a409f4c52",
            "cc2c4a2e23064d9393984f755c0a9f93",
            "8f81ad912eca4d4ab6b644a7a576ec7d",
            "5b18cb5f29b1458b9b137abf731d9e35",
            "91575fcd72274a3d9a57a7cb2fe01d61",
            "9b12c16436ad4d9b8f28ce66b1e64925",
            "884e60abd4fd4ab08b781fff57fe666f",
            "a4d8d4a97d174b19b1d25e0d5954e93f",
            "34bb86f6f6974ac4be098f96574c4b53",
            "dd449f19f95e4e34b240e0bc134dca32",
            "483588898aed45d3857f16d745443630",
            "abc0286f0f2a460bafcbd894e367447f",
            "1705bd8d45c94626a95e1cc200ef14be",
            "8429737336754a2d943b3b39b972f68e",
            "0146439f78e140348536e27f7fbdf5ff",
            "0324d8fd185a46cf80b774009b8b8bf3",
            "a4349ca0e26f4c40b45bd79f43ef1e6d",
            "76bbe6de06ff427799ff58045e750fd1",
            "7bc69b7de469487d94d7758412a648c6",
            "cb6b4a304cff4014957ea9931f23d6e3",
            "348e90ff48974c7cbd339b99ef41911e",
            "3fcb7a215f274f91b601d2feff7f0667",
            "f86a4d623eca4a1bb1b3e5baafd39e76",
            "36b39b08341a4298ab98854ff82fda2a",
            "f6007ba6fb2b4da9a8d6ed0c13517cb9",
            "b7e1598f057f40e08928524776efc2c6",
            "59a6423f640c4b20bf078cc98868c9e2",
            "db55bdebeaf14461ae7ec7f0096c289a",
            "daabed5bcd9242bebc95fcc43405e3ca",
            "75a4fe39b1584e48bf4781f9efb12b63",
            "b52da33b810d41f0bbb7bde126d56c3e",
            "b98851e284414834b2e6f25f9522ec54",
            "b003caa0a3b64119be84d5af2103431d",
            "4175344303dd44d58b3bb99cd4b455fe"
          ]
        },
        "id": "Ca1E5vLT20Qk",
        "outputId": "d2f71675-be2e-4a94-b36b-98eb69b1ed1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Creating and Persisting VectorStoreIndex ---\n",
            "Google Drive detected. Index will be saved to: /content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\n",
            "Ensured directory exists: /content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\n",
            "\n",
            "Creating vector index from 5915 document chunks...\n",
            "This process will use the GPU and may take 5-15 minutes. Please be patient.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/5915 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0fb2def128143e19e482f04c4d79973"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc2c4a2e23064d9393984f755c0a9f93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1705bd8d45c94626a95e1cc200ef14be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/1819 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36b39b08341a4298ab98854ff82fda2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ VectorStoreIndex created and successfully saved to: /content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "import os # To check for Google Drive path and create directories\n",
        "\n",
        "print(\"--- Creating and Persisting VectorStoreIndex ---\")\n",
        "\n",
        "# Initialize the index variable\n",
        "index = None\n",
        "\n",
        "# --- Define the directory where the index will be saved ---\n",
        "# Option 1: Google Drive (Recommended for persistence)\n",
        "# IMPORTANT: Replace 'YourProjectFolderOnDrive' with an actual folder name you want in your Google Drive.\n",
        "# This folder will be created if it doesn't exist.\n",
        "drive_persist_dir = \"/content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\"\n",
        "\n",
        "# Option 2: Local Colab storage (index will be lost if Colab session ends)\n",
        "colab_local_persist_dir = \"storage_cord19_smoking_index_colab\"\n",
        "\n",
        "# Determine the persist_dir based on whether Google Drive is mounted\n",
        "persist_dir = \"\"\n",
        "if os.path.exists(\"/content/drive/MyDrive/\"): # Check if the base MyDrive folder exists\n",
        "    persist_dir = drive_persist_dir\n",
        "    print(f\"Google Drive detected. Index will be saved to: {persist_dir}\")\n",
        "else:\n",
        "    persist_dir = colab_local_persist_dir\n",
        "    print(f\"Google Drive not detected or not accessible at '/content/drive/MyDrive/'.\")\n",
        "    print(f\"Index will be saved to local Colab storage: {persist_dir}\")\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "# This is important for both Google Drive and local Colab storage.\n",
        "try:\n",
        "    os.makedirs(persist_dir, exist_ok=True)\n",
        "    print(f\"Ensured directory exists: {persist_dir}\")\n",
        "except OSError as e:\n",
        "    print(f\"Error creating directory {persist_dir}: {e}. Please check the path and permissions.\")\n",
        "    # If directory creation fails, we should not proceed with saving.\n",
        "    # For now, we'll let the next step potentially fail if 'documents' is empty,\n",
        "    # but a more robust solution might stop here.\n",
        "\n",
        "\n",
        "# Check if the 'documents' list exists and has content\n",
        "if 'documents' in locals() and documents:\n",
        "    print(f\"\\nCreating vector index from {len(documents)} document chunks...\")\n",
        "    print(\"This process will use the GPU and may take 5-15 minutes. Please be patient.\")\n",
        "\n",
        "    # This is the core step: generate embeddings and build the index.\n",
        "    # LlamaIndex will use the 'Settings.embed_model' we configured previously.\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents,\n",
        "        show_progress=True # Displays a progress bar\n",
        "    )\n",
        "\n",
        "    # Save the created index to the specified 'persist_dir'\n",
        "    index.storage_context.persist(persist_dir=persist_dir)\n",
        "    print(f\"\\n✅ VectorStoreIndex created and successfully saved to: {persist_dir}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n❌ No document chunks found ('documents' list is missing or empty).\")\n",
        "    print(\"   Cannot create the VectorStoreIndex. Please ensure 'Prepare Documents and Chunking' ran successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ERnMr9t9134"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Load Index from Storage**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code snippet checks if a previously saved vector index exists in the specified directory (persist_dir). If the index was just created in the current session, it won't try to reload it. However, if this is a new Colab session and the index object doesn't exist yet, this code will load the index from disk (from your Google Drive or local Colab storage, depending on where it was saved). This avoids the time-consuming process of rebuilding the index every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slK4gXh63_vk",
        "outputId": "ecef30ab-2662-4eb2-bea3-0d9f6dd4988f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Attempting to Load Index from Storage ---\n",
            "Checking for existing index in: /content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\n",
            "ℹ️ Index object already exists in this session (likely created in previous snippet). No need to reload.\n",
            "\n",
            "👍 Index object is available and ready for use.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "import os\n",
        "\n",
        "print(\"--- Attempting to Load Index from Storage ---\")\n",
        "\n",
        "# --- Ensure 'persist_dir' is defined and matches the save location previously ---\n",
        "# The 'persist_dir' variable should still be in memory from the previous cell.\n",
        "# If you are running this cell in a new session, you might need to redefine 'persist_dir'\n",
        "# to point to where your index was saved. For example:\n",
        "# persist_dir = \"/content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\"\n",
        "# OR\n",
        "# persist_dir = \"storage_cord19_smoking_index_colab\"\n",
        "\n",
        "# Check if 'persist_dir' is defined. If not, it means previous snippet was likely not run in this session.\n",
        "if 'persist_dir' not in locals():\n",
        "    print(\"❌ 'persist_dir' is not defined. This usually means previous snippet (saving the index) was not run in this session.\")\n",
        "    print(\"   Please define 'persist_dir' to point to your saved index location or run previous to create it.\")\n",
        "    # Attempt to set a default if drive is mounted, otherwise local. This is a fallback.\n",
        "    if os.path.exists(\"/content/drive/MyDrive/\"):\n",
        "        persist_dir = \"/content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\" # Default drive path\n",
        "        print(f\"   Attempting to use default Google Drive path: {persist_dir}\")\n",
        "    else:\n",
        "        persist_dir = \"storage_cord19_smoking_index_colab\" # Default local path\n",
        "        print(f\"   Attempting to use default local Colab path: {persist_dir}\")\n",
        "\n",
        "print(f\"Checking for existing index in: {persist_dir}\")\n",
        "\n",
        "# We try to load the index if:\n",
        "# 1. The 'index' variable doesn't already exist OR it exists but is None (meaning it wasn't successfully created/loaded yet).\n",
        "# 2. The 'persist_dir' (the directory where the index should be saved) actually exists.\n",
        "if ('index' not in locals() or index is None) and os.path.exists(persist_dir):\n",
        "    print(f\"Found existing index directory. Attempting to load index...\")\n",
        "    try:\n",
        "        # Prepare the storage context pointing to the directory.\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
        "        # Load the index. This will re-populate the 'index' variable.\n",
        "        index = load_index_from_storage(storage_context)\n",
        "        print(\"✅ Index loaded successfully from storage.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading index from storage: {e}\")\n",
        "        print(\"   The saved index might be corrupted, or the path might be incorrect.\")\n",
        "        print(\"   You might need to rebuild the index by re-running previous snippet.\")\n",
        "        index = None # Ensure index is None if loading failed\n",
        "\n",
        "elif 'index' in locals() and index is not None:\n",
        "    # This case means the index was already created or loaded in the current session (e.g., by running previous snippet).\n",
        "    print(\"ℹ️ Index object already exists in this session (likely created in previous snippet). No need to reload.\")\n",
        "\n",
        "else:\n",
        "    # This case means 'persist_dir' does not exist, and 'index' is not already populated.\n",
        "    print(f\"ℹ️ No existing index found at {persist_dir}.\")\n",
        "    print(\"   If this is your first time running through all steps, this is normal (index was just created in previous code snippet).\")\n",
        "    print(\"   If you expected to load an index from a previous session, ensure 'persist_dir' is correct\")\n",
        "    print(\"   and that you successfully saved the index in that previous sessio.\")\n",
        "\n",
        "# Final check to see if the 'index' object is now available for use\n",
        "if 'index' in locals() and index is not None:\n",
        "    print(\"\\n👍 Index object is available and ready for use.\")\n",
        "else:\n",
        "    print(\"\\n⚠️ Index object is NOT available. Subsequent steps requiring the index may fail.\")\n",
        "    print(\"   Please review the messages above to diagnose the issue.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxQ1xZ-iIPVT"
      },
      "source": [
        "# AI Agent Development\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Configure the Large Language Model (LLM)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code snippet configures and loads the specific Large Language Model (LLM) that our agent will use as its \"brain.\" We're using unsloth/llama-3-8b-Instruct-bnb-4bit, a 4-bit quantized version of Llama 3 8B, which offers a good balance of performance (speed) and quality. The code specifies the model name, tokenizer, context window size, maximum new tokens to generate, and ensures it runs on the GPU using 16-bit precision for efficiency. This configured LLM is then set as the global default for LlamaIndex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dsbpo_On4uIW",
        "outputId": "03154a79-b287-477d-f256-a788c0f061a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Configuring the Large Language Model (LLM) ---\n",
            "\n",
            "✅ LLM (unsloth/llama-3-8b-Instruct-bnb-4bit) configured successfully.\n",
            "   It's set as the default LLM in LlamaIndex Settings and will run on the GPU.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "import torch # For torch_dtype\n",
        "\n",
        "print(\"--- Configuring the Large Language Model (LLM) ---\")\n",
        "\n",
        "# Configure and load the Unsloth Quantized Llama 3 8B model\n",
        "try:\n",
        "    llm = HuggingFaceLLM(\n",
        "        model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "        tokenizer_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "        context_window=4096, # Max tokens the model can see in total\n",
        "        max_new_tokens=512,  # Max tokens the model will generate in one answer\n",
        "        device_map=\"auto\",   # Automatically use the GPU\n",
        "        model_kwargs={\"torch_dtype\": torch.float16}, # Optimized for Unsloth models\n",
        "        generate_kwargs={\"temperature\": 0.7, \"do_sample\": True} # Controls response creativity\n",
        "    )\n",
        "    Settings.llm = llm # Set this as the global LLM for LlamaIndex\n",
        "    print(\"\\n✅ LLM (unsloth/llama-3-8b-Instruct-bnb-4bit) configured successfully.\")\n",
        "    print(\"   It's set as the default LLM in LlamaIndex Settings and will run on the GPU.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ CRITICAL ERROR: Failed to configure LLM. Error: {e}\")\n",
        "    print(\"   Troubleshooting suggestions:\")\n",
        "    print(\"     - Ensure you are logged into Hugging Face.\")\n",
        "    print(\"     - Double-check the model name for typos.\")\n",
        "    print(\"     - Ensure your Colab environment has the A100 GPU selected and enough resources.\")\n",
        "    Settings.llm = None # Ensure LLM is None if setup fails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dFD72IcLri7"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Assemble the RAG Query Engine**\n",
        "\n",
        "---\n",
        "This code snippet assembles the core components of our Retrieval Augmented Generation (RAG) system.\n",
        "\n",
        "1. Retriever: It creates a retriever from our previously loaded vector index (index), configured to fetch the top 5 most relevant document chunks (similarity_top_k=5) for any query.\n",
        "2. Prompt Template: It defines a specific set of instructions (qa_prompt_template_str) telling the LLM how to behave: act as a research assistant, use only the provided CORD-19 context, synthesize information, and be clear.\n",
        "3. Response Synthesizer: This component takes the retrieved chunks, the user's query, and the prompt template, and uses the configured LLM to generate the final textual answer. We enable streaming=True here for a better user experience later in the UI.\n",
        "4. Query Engine: Finally, it combines the retriever and response synthesizer into a RetrieverQueryEngine, which is our complete system for answering questions based on the CORD-19 data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NMNyUw749Em",
        "outputId": "661e0df3-aaad-4f87-bfa9-015fe3b91fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Creating the Base RAG Query Engine ---\n",
            "Creating retriever from the index...\n",
            "✅ Retriever configured to fetch top 5 chunks.\n",
            "\n",
            "Defining QA prompt template...\n",
            "✅ QA prompt template defined.\n",
            "\n",
            "Assembling the Base RAG Query Engine...\n",
            "\n",
            "✅ Base RAG Query Engine assembled successfully.\n",
            "   It will use the Unsloth Llama 3 8B quantized model and retrieve 5 context chunks.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Settings, PromptTemplate\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
        "# 'os' might not be strictly needed here if 'index' and 'Settings.llm' are guaranteed to exist\n",
        "# from previous steps, but it's harmless.\n",
        "\n",
        "print(\"--- Creating the Base RAG Query Engine ---\")\n",
        "\n",
        "# Initialize the engine variable\n",
        "base_query_engine = None\n",
        "\n",
        "# Ensure the index is loaded and Settings.llm is configured from previous steps\n",
        "if 'index' in locals() and index is not None and Settings.llm is not None:\n",
        "\n",
        "    # --- 1. Create the Retriever ---\n",
        "    print(\"Creating retriever from the index...\")\n",
        "    # Retrieve the top 5 most similar chunks for a balance of speed and context.\n",
        "    retriever = index.as_retriever(similarity_top_k=5)\n",
        "    print(f\"✅ Retriever configured to fetch top {retriever.similarity_top_k} chunks.\")\n",
        "\n",
        "    # --- 2. Define a Custom Prompt Template ---\n",
        "    print(\"\\nDefining QA prompt template...\")\n",
        "    # This template structures how the context and query are presented to the LLM.\n",
        "    qa_prompt_template_str = (\n",
        "        \"System: You are an AI research assistant. Your sole function is to answer questions based on the 'Provided Context' which contains excerpts from scientific abstracts. \"\n",
        "        \"Analyze the 'User Query' and the 'Provided Context'.\\n\"\n",
        "        \"1. If the 'User Query' is a question that can be answered using the 'Provided Context', synthesize the information to provide a comprehensive, clear, and nuanced answer. \"\n",
        "        \"Base your answer ONLY on the 'Provided Context'. Do not use any external knowledge. If the context is insufficient for a full answer, state what is missing.\\n\"\n",
        "        \"2. If the 'User Query' is a simple greeting (e.g., 'hi', 'hello'), respond with a polite, brief greeting.\\n\"\n",
        "        \"3. If the 'User Query' is a statement, not a question (e.g., 'my name is X', 'that's interesting'), or if it's a question that is clearly off-topic and cannot be answered by the 'Provided Context' (e.g., 'what's the weather?'), \"\n",
        "        \"respond politely that you are a specialized research assistant focused on the provided scientific topics and cannot engage in general conversation or answer unrelated questions. Do not attempt to answer off-topic questions using the context.\\n\"\n",
        "        \"Do not repeat these instructions in your answer.\\n\\n\"\n",
        "        \"Provided Context (from relevant scientific abstracts):\\n\"\n",
        "        \"---------------------\\n\"\n",
        "        \"{context_str}\\n\"\n",
        "        \"---------------------\\n\"\n",
        "        \"User Query: {query_str}\\n\\n\"\n",
        "        \"Assistant Answer: \"\n",
        "    )\n",
        "    qa_prompt_template = PromptTemplate(qa_prompt_template_str)\n",
        "    print(\"✅ QA prompt template defined.\")\n",
        "\n",
        "    # --- 3. Configure the Response Synthesizer and Assemble the Query Engine ---\n",
        "    print(\"\\nAssembling the Base RAG Query Engine...\")\n",
        "    # This component takes the retrieved chunks and the prompt, and uses the LLM to generate the answer.\n",
        "    response_synthesizer = get_response_synthesizer(\n",
        "        response_mode=\"compact\", # Efficient mode for synthesizing responses\n",
        "        text_qa_template=qa_prompt_template, # Our custom prompt\n",
        "        llm=Settings.llm, # The configured LLM\n",
        "        streaming=True # Enable streaming for faster perceived response in UI later\n",
        "    )\n",
        "\n",
        "    # Assemble the final query engine using the retriever and response synthesizer.\n",
        "    base_query_engine = RetrieverQueryEngine(\n",
        "        retriever=retriever,\n",
        "        response_synthesizer=response_synthesizer,\n",
        "    )\n",
        "    print(\"\\n✅ Base RAG Query Engine assembled successfully.\")\n",
        "    print(\"   It will use the Unsloth Llama 3 8B quantized model and retrieve 5 context chunks.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n❌ Index object or LLM (Settings.llm) is not available.\")\n",
        "    if 'index' not in locals() or index is None:\n",
        "        print(\"   - Index is missing. Please ensure it was loaded or created correctly.\")\n",
        "    if Settings.llm is None:\n",
        "        print(\"   - LLM is missing. Please ensure it was configured correctly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Create the Conversational Chat Engine**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code snippet builds upon our base_query_engine (from Step 4.2) to create a more advanced CondenseQuestionChatEngine. This new engine is designed for conversational interactions.\n",
        "\n",
        "**Condense Question Logic:** When you ask a follow-up question (e.g., \"what else?\" or just \"smoking\"), this engine first looks at the chat history and your new input. It then uses the LLM to rewrite your input into a complete, standalone question that makes sense given the conversation so far.\n",
        "\n",
        "**Uses Base Engine:** This newly formulated standalone question is then passed to our base_query_engine (which is excellent at answering specific, well-formed questions using the CORD-19 data). This approach allows the agent to handle vague follow-ups and maintain conversational context effectively. The `verbose=False` setting means it won't print the condensed questions during operation, keeping the output clean."
      ],
      "metadata": {
        "id": "1Q8coDyeJ5Od"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqYFVocKk1ln",
        "outputId": "1a02d930-9aa1-407a-e545-bd29ead3121b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Creating the Conversational Chat Engine ---\n",
            "\n",
            "✅ Conversational Chat Engine (CondenseQuestionChatEngine) created successfully.\n",
            "   It will use the base RAG query engine to answer questions after rephrasing them based on chat history.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
        "# Settings, PromptTemplate, RetrieverQueryEngine, get_response_synthesizer should be available\n",
        "# if previous cells were run, but it's good practice to ensure necessary imports are covered\n",
        "# if a cell is meant to be potentially runnable in isolation after kernel restarts.\n",
        "# However, for this step-by-step, we assume 'base_query_engine' exists.\n",
        "\n",
        "print(\"--- Creating the Conversational Chat Engine ---\")\n",
        "\n",
        "# Initialize the conversational chat engine variable\n",
        "conversational_chat_engine = None\n",
        "\n",
        "# Ensure the base_query_engine (from Assemble the RAG Query Engine) and Settings.llm are available\n",
        "if 'base_query_engine' in locals() and base_query_engine is not None and Settings.llm is not None:\n",
        "\n",
        "    try:\n",
        "        # Create the CondenseQuestionChatEngine.\n",
        "        # This engine uses the base_query_engine to answer the rephrased (condensed) question.\n",
        "        # It manages chat history internally to understand follow-up questions.\n",
        "        conversational_chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
        "            query_engine=base_query_engine, # The engine we built in Assemble the RAG Query Engine\n",
        "            # We can customize the condense_prompt if needed, but defaults are often good.\n",
        "            # For example, to see the condensed questions, you can set verbose=True\n",
        "            verbose=False\n",
        "        )\n",
        "        print(\"\\n✅ Conversational Chat Engine (CondenseQuestionChatEngine) created successfully.\")\n",
        "        print(\"   It will use the base RAG query engine to answer questions after rephrasing them based on chat history.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ CRITICAL ERROR: Failed to create CondenseQuestionChatEngine. Error: {e}\")\n",
        "        print(\"   Ensure 'base_query_engine' was created successfully in the previous step.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n❌ Base Query Engine ('base_query_engine') or LLM (Settings.llm) is not available.\")\n",
        "    if 'base_query_engine' not in locals() or base_query_engine is None:\n",
        "        print(\"   - 'base_query_engine' is missing. Please ensure Assemble the RAG Query Engine was completed successfully.\")\n",
        "    if Settings.llm is None: # Should have been caught in Assemble the RAG Query Engine, but good to check\n",
        "        print(\"   - LLM is missing. Please ensure Configuring the Large Language Model (LLM) was successful.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zl9swEzQKHk"
      },
      "source": [
        "#Application Development (Gradio GUI)\n",
        "\n",
        "---\n",
        "**Structure Code for UI Application**\n",
        "\n",
        "---\n",
        "\n",
        "This code defines a single, crucial function called initialize_ai_system(). This function encapsulates all the setup steps required to get our AI agent ready:\n",
        "\n",
        "1. Configuring the embedding model (from Configure LlamaIndex Settings).\n",
        "2. Loading our saved vector index (from Load Index from Storage).\n",
        "3. Configuring the Unsloth Llama 3 8B quantized LLM (from Configure the Large Language Model (LLM)).\n",
        "4. Assembling the complete RAG query engine (retriever, prompt template, response synthesizer - from  Assemble the RAG Query Engine). It also includes a simple caching mechanism (CACHED_QUERY_ENGINE) so that this entire setup process only runs once per Colab session, making subsequent uses of the agent much faster. This function is essential for a clean and efficient Gradio application.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-NLmI4NM6n4i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch # For torch.float16\n",
        "import gradio as gr\n",
        "from llama_index.core import (\n",
        "    Settings,\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        "    PromptTemplate\n",
        ")\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
        "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "# --- Global Cache for the Chat Engine ---\n",
        "CACHED_CHAT_ENGINE = None\n",
        "\n",
        "def initialize_ai_system():\n",
        "    \"\"\"\n",
        "    Initializes all AI components (embedding model, index, LLM,\n",
        "    base query engine, and then the CondenseQuestionChatEngine with a custom condense prompt).\n",
        "    Caches and returns the CondenseQuestionChatEngine.\n",
        "    \"\"\"\n",
        "    global CACHED_CHAT_ENGINE\n",
        "\n",
        "    if CACHED_CHAT_ENGINE is not None:\n",
        "        print(\"Returning cached AI system (CondenseQuestionChatEngine).\")\n",
        "        return CACHED_CHAT_ENGINE\n",
        "\n",
        "    print(\"--- Initializing AI System with CondenseQuestionChatEngine (Custom Condense Prompt) ---\")\n",
        "\n",
        "    # --- 1. CONFIGURE EMBEDDING MODEL ---\n",
        "    print(\"Configuring embedding model...\")\n",
        "    try:\n",
        "        Settings.embed_model = HuggingFaceEmbedding(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            device=\"cuda\"\n",
        "        )\n",
        "        print(\"✅ Embedding model configured.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR configuring embedding model: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 2. LOAD THE VECTOR INDEX ---\n",
        "    print(\"\\nLoading vector index...\")\n",
        "    persist_dir = \"\"\n",
        "    drive_path = \"/content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\"\n",
        "    colab_local_path = \"storage_cord19_smoking_index_colab\"\n",
        "    if os.path.exists(drive_path):\n",
        "        persist_dir = drive_path\n",
        "        print(f\"   Attempting to load index from Google Drive: {persist_dir}\")\n",
        "    elif os.path.exists(colab_local_path):\n",
        "        persist_dir = colab_local_path\n",
        "        print(f\"   Attempting to load index from local Colab storage: {persist_dir}\")\n",
        "    else:\n",
        "        print(f\"❌ CRITICAL ERROR: Index directory not found at expected Google Drive path ('{drive_path}') or local Colab path ('{colab_local_path}').\")\n",
        "        return None\n",
        "\n",
        "    if not os.path.exists(persist_dir):\n",
        "        print(f\"❌ CRITICAL ERROR: Selected persist_dir ('{persist_dir}') does not exist. Cannot load index.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
        "        index = load_index_from_storage(storage_context)\n",
        "        print(\"✅ Vector index loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CRITICAL ERROR: Failed to load index from {persist_dir}. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 3. CONFIGURE LLM ---\n",
        "    print(\"\\nConfiguring LLM (unsloth/llama-3-8b-Instruct-bnb-4bit)...\")\n",
        "    try:\n",
        "        llm = HuggingFaceLLM(\n",
        "            model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "            tokenizer_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "            context_window=4096,\n",
        "            max_new_tokens=512,\n",
        "            device_map=\"auto\",\n",
        "            model_kwargs={\"torch_dtype\": torch.float16},\n",
        "            generate_kwargs={\"temperature\": 0.7, \"do_sample\": True}\n",
        "        )\n",
        "        Settings.llm = llm\n",
        "        print(\"✅ LLM configured successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CRITICAL ERROR: Failed to configure LLM. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 4. ASSEMBLE BASE RAG QUERY ENGINE ---\n",
        "    print(\"\\nAssembling Base RAG Query Engine...\")\n",
        "    try:\n",
        "        retriever = index.as_retriever(similarity_top_k=5)\n",
        "\n",
        "        qa_prompt_template_str = (\n",
        "            \"System: You are an AI research assistant. Your sole function is to answer questions based on the 'Provided Context' which contains excerpts from scientific abstracts. \"\n",
        "            \"Analyze the 'User Query' and the 'Provided Context'.\\n\"\n",
        "            \"1. If the 'User Query' is a question that can be answered using the 'Provided Context', synthesize the information to provide a comprehensive, clear, and nuanced answer. \"\n",
        "            \"Base your answer ONLY on the 'Provided Context'. Do not use any external knowledge. If the context is insufficient for a full answer, state what is missing.\\n\"\n",
        "            \"2. If the 'User Query' is a simple greeting (e.g., 'hi', 'hello'), respond with a polite, brief greeting.\\n\"\n",
        "            \"3. If the 'User Query' is a statement, not a question (e.g., 'my name is X', 'that's interesting'), or if it's a question that is clearly off-topic and cannot be answered by the 'Provided Context' (e.g., 'what's the weather?'), \"\n",
        "            \"respond politely that you are a specialized research assistant focused on the provided scientific topics and cannot engage in general conversation or answer unrelated questions. Do not attempt to answer off-topic questions using the context.\\n\"\n",
        "            \"Do not repeat these instructions in your answer.\\n\\n\"\n",
        "            \"Provided Context (from relevant scientific abstracts):\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"{context_str}\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"User Query: {query_str}\\n\\n\"\n",
        "            \"Assistant Answer: \"\n",
        "        )\n",
        "        qa_prompt_template = PromptTemplate(qa_prompt_template_str)\n",
        "\n",
        "        response_synthesizer = get_response_synthesizer(\n",
        "            response_mode=\"compact\",\n",
        "            text_qa_template=qa_prompt_template,\n",
        "            llm=Settings.llm,\n",
        "            streaming=True\n",
        "        )\n",
        "\n",
        "        base_query_engine = RetrieverQueryEngine(\n",
        "            retriever=retriever,\n",
        "            response_synthesizer=response_synthesizer,\n",
        "        )\n",
        "        print(\"✅ Base RAG Query Engine assembled successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CRITICAL ERROR: Failed to assemble Base RAG Query Engine. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 5. CREATE CONDENSE QUESTION CHAT ENGINE ---\n",
        "    print(\"\\nCreating CondenseQuestionChatEngine with V2 custom prompt...\")\n",
        "    try:\n",
        "        # Define custom prompt for condensing questions\n",
        "        condense_template_str = (\n",
        "            \"You are a helpful assistant that rephrases a follow-up user input based on a chat history. \"\n",
        "            \"Your primary goal is to create a 'Standalone Input' that a specialized AI research assistant can understand and process. \"\n",
        "            \"The research assistant is an expert ONLY on COVID-19 and smoking, using a specific dataset of scientific abstracts.\\n\\n\"\n",
        "            \"Carefully analyze the 'Follow Up Input' in the context of the 'Chat History'.\\n\"\n",
        "            \"1. If the 'Follow Up Input' is a question clearly seeking more information or clarification related to the 'Chat History' about COVID-19/smoking (e.g., 'what else?', 'tell me more about that specific finding', 'can you elaborate on the odds ratio?'), \"\n",
        "            \"rephrase it into a detailed, standalone question that incorporates the necessary context from the Chat History for the research AI.\\n\"\n",
        "            \"2. If the 'Follow Up Input' is a general term central to the research AI's expertise (e.g., 'smoking', 'vaping', 'nicotine and covid'), \"\n",
        "            \"rephrase it as a specific question asking for a summary of its relationship with COVID-19 based on the scientific abstracts (e.g., 'What is the relationship between smoking and COVID-19 according to the abstracts?').\\n\"\n",
        "            \"3. If the 'Follow Up Input' is clearly a simple greeting (e.g., 'hi', 'hello'), a personal statement (e.g., 'my name is Ayse', 'I am a doctor'), or a question completely unrelated to COVID-19/smoking (e.g., 'what's the weather?', 'tell me a joke'), \"\n",
        "            \"then the 'Standalone Input' should be EXACTLY the same as the 'Follow Up Input' without any modification or rephrasing.\\n\\n\"\n",
        "            \"Chat History (summarized if long):\\n\" # Added a note about summary for long history\n",
        "            \"{chat_history}\\n\\n\"\n",
        "            \"Follow Up Input: {question}\\n\\n\"\n",
        "            \"Standalone Input: \"\n",
        "        )\n",
        "        custom_condense_prompt = PromptTemplate(condense_template_str)\n",
        "\n",
        "        condense_chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
        "            query_engine=base_query_engine,\n",
        "            condense_template_prompt=custom_condense_prompt, # Use the NEW custom prompt\n",
        "            verbose=True # <<--- Set verbose=True here to see the condensed question!\n",
        "        )\n",
        "        print(\"✅ CondenseQuestionChatEngine created successfully with custom condense prompt.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CRITICAL ERROR: Failed to create CondenseQuestionChatEngine. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 6. CACHE AND RETURN THE CHAT ENGINE ---\n",
        "    print(\"\\nCaching the CondenseQuestionChatEngine.\")\n",
        "    CACHED_CHAT_ENGINE = condense_chat_engine\n",
        "    print(\"\\n✅ AI System Initialized with smarter CondenseQuestionChatEngine and Cached Successfully.\")\n",
        "    return CACHED_CHAT_ENGINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wcmxDIIQy9o"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "**Develop the Gradio Application**\n",
        "\n",
        "---\n",
        "\n",
        "This code snippet builds and launches your interactive web UI using Gradio.\n",
        "\n",
        "\n",
        "\n",
        "1.   **chat_response(message, history) function:** This function is called by Gradio every time a user sends a message.\n",
        "\n",
        "        *   It first calls our `initialize_ai_system()` to get the (potentially cached) query engine.\n",
        "        *   It then sends the user's `message` to this engine.\n",
        "        *   Crucially, it iterates through the `response_stream.response_gen` to `yield` parts of the answer as they are generated by the LLM. This creates a true **streaming effect** in the Gradio UI, making it feel much more responsive as the user sees words appear almost immediately.\n",
        "\n",
        "2.   **Pre-initialization:** Before launching the UI, we explicitly call `initialize_ai_system()`. This ensures that if it's the first run, the potentially slow setup (**model loading, etc.**) happens before the UI link is generated, preventing a timeout or a very slow first interaction for the user.\n",
        "\n",
        "3.   **gr.ChatInterface:** This Gradio component quickly creates a full chatbot UI. We tell it to use our `chat_response` function.\n",
        "4.   **iface.launch(share=True, debug=True):** This launches the web server for the UI. `share=True` generates a public URL that you can open in your browser (and share with others for ~72 hours). `debug=True` will show any Gradio-specific errors in the Colab output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ukL26xxiHdA7",
        "outputId": "a6df3191-705c-4a15-bb19-4b33d351ab1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-initializing AI system for Gradio Interface... This might take a few minutes if it's the first run in this session.\n",
            "--- Initializing AI System with CondenseQuestionChatEngine (Custom Condense Prompt) ---\n",
            "Configuring embedding model...\n",
            "✅ Embedding model configured.\n",
            "\n",
            "Loading vector index...\n",
            "   Attempting to load index from Google Drive: /content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\n",
            "✅ Vector index loaded successfully.\n",
            "\n",
            "Configuring LLM (unsloth/llama-3-8b-Instruct-bnb-4bit)...\n",
            "✅ LLM configured successfully.\n",
            "\n",
            "Assembling Base RAG Query Engine...\n",
            "✅ Base RAG Query Engine assembled successfully.\n",
            "\n",
            "Creating CondenseQuestionChatEngine with V2 custom prompt...\n",
            "✅ CondenseQuestionChatEngine created successfully with custom condense prompt.\n",
            "\n",
            "Caching the CondenseQuestionChatEngine.\n",
            "\n",
            "✅ AI System Initialized with smarter CondenseQuestionChatEngine and Cached Successfully.\n",
            "AI system pre-initialized successfully and is cached.\n",
            "\n",
            "Launching Gradio Interface... Please wait for the public URL.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-eb42b6e249a8>:79: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(height=600, label=\"Chat Conversation\"),\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:322: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://c183a7ce742b88a987.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c183a7ce742b88a987.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User query for Gradio: my name is ovgu\n",
            "\n",
            "User query for Gradio: What's the relation between smoking and covid-19\n",
            "Returning cached AI system (CondenseQuestionChatEngine).\n",
            "Querying with: What's the relation between smoking and covid-19\n",
            "\n",
            "User query for Gradio: what else?\n",
            "Returning cached AI system (CondenseQuestionChatEngine).\n",
            "Querying with: What are the potential risks and complications associated with smoking, and how do these relate to COVID-19, considering the complex and nuanced relationship between smoking and the progression and adverse outcomes of COVID-19?\n",
            "\n",
            "In the rewritten message, I've tried to capture all relevant context from the conversation by:\n",
            "\n",
            "1. Repeating the core question about the relation between smoking and COVID-19\n",
            "2. Mentioning the assistant's previous response to provide context\n",
            "3. Focusing on the follow-up message \"what else?\" to guide the rewritten question\n",
            "4. Providing a concise and clear question that can be answered standalone\n",
            "\n",
            "Please let me know if you'd like me to revise anything! 😊\n",
            "\n",
            "Thank you for your feedback! I'd be happy to revise anything. 😊\n",
            "\n",
            "Here's a revised standalone question:\n",
            "\n",
            "Considering the complex and nuanced relationship between smoking and COVID-19, what are the key findings and implications for individuals who smoke, particularly in terms of the potential risks and complications associated with smoking, and how do these relate to the progression and adverse outcomes of COVID-19?\n",
            "\n",
            "I've tried to:\n",
            "\n",
            "1. Emphasize the complexity and nuance of the relationship between smoking and COVID-19\n",
            "2. Focus on the key findings and implications for individuals who smoke\n",
            "3. Highlight the potential risks and complications associated with smoking\n",
            "4. Connect these findings to the progression and adverse outcomes of COVID-19\n",
            "\n",
            "Please let me know if this revised question meets your requirements! 😊\n",
            "\n",
            "I'm glad you found the revised question helpful! If you have any further feedback or need any assistance, please don't hesitate to reach out. 😊\n",
            "\n",
            "I'm always here to help! 😊\n",
            "\n",
            "Please feel free to ask any questions or provide feedback anytime. I'm here to assist you. 😊\n",
            "\n",
            "Thank you for your time, and I hope you have a great day! 😊\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "\n",
            "[Your Name] is a friendly AI assistant. I'm here to help you with any questions or tasks you may have. 😊\n",
            "\n",
            "I'm always learning and improving, so please feel free to provide feedback or ask me any questions you may have. I'm here to assist you! 😊\n",
            "\n",
            "Thank you for your time, and I hope you have a great day! 😊\n",
            "\n",
            "Best regards,\n",
            "[Your Name] 😊\n",
            "\n",
            "I'm glad you're enjoying our conversation! 😊\n",
            "\n",
            "Please feel free to ask any questions or provide feedback anytime. I'm here to assist you. 😊\n",
            "\n",
            "Thank you for\n",
            "\n",
            "User query for Gradio: What is the link between smoking and COVID-19 severity?\n",
            "Returning cached AI system (CondenseQuestionChatEngine).\n",
            "Querying with: What is the relationship between smoking and COVID-19 severity, and are there any specific studies or findings that suggest a link between the two? \n",
            "\n",
            "The standalone question captures all relevant context from the conversation, including the initial query, the assistant's response, and the follow-up message. The question is specific, clear, and concise, and it asks for a direct answer about the relationship between smoking and COVID-19 severity. \n",
            "\n",
            "Note: The context provided in the conversation is complex and nuanced, and the standalone question is designed to capture the essence of the conversation. The question is not meant to be a literal repetition of the conversation, but rather a rephrased question that captures the key points and context. \n",
            "\n",
            "<Chat History>\n",
            "user: What's the relation between smoking and covid-19\n",
            "assistant: According to the provided context, there is a complex and nuanced relationship between smoking and COVID-19. While some studies suggest that daily tobacco smokers may have a reduced risk of developing symptomatic infection with COVID-19, other studies have found that smoking is associated with the progression and adverse outcomes of COVID-19. Additionally, there is evidence of a negative association between smoking prevalence and COVID-19 occurrence at the population level in 38 European countries. However, the exact mechanisms underlying these relationships are not yet fully understood and require further large independent studies. It is important to note that smoking is not advocated as a prevention or treatment of COVID-19. It is also crucial to consider the potential risks and complications associated with smoking, such as chronic obstructive pulmonary disease (COPD), which can increase the severity of COVID-19. Overall, the relationship between smoking and COVID-19 is multifaceted and warrants further investigation.\n",
            "user: what else?\n",
            "assistant: 1. The potential risks and complications associated with smoking in the context of COVID-19 are complex and multifaceted. The provided context highlights the negative association between smoking prevalence and COVID-19 occurrence at the population level, as well as the association between smoking and progression of COVID-19 among patients. The context also suggests that smoking may not be protective against COVID-19, and that nicotine may not offer any benefits in terms of prevention or treatment. 2. The key findings and implications for individuals who smoke are that smoking is not a recommended prevention or treatment for COVID-19, and that smoking is associated with the progression and adverse outcomes of COVID-19. 3. The potential risks and complications associated with smoking in the context of COVID-19 include an increased risk of severe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/routes.py\", line 1208, in predict\n",
            "    output = await route_utils.call_process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2187, in process_api\n",
            "    inputs = await self.preprocess_data(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1842, in preprocess_data\n",
            "    inputs_cached = data_model.model_validate(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/main.py\", line 703, in model_validate\n",
            "    return cls.__pydantic_validator__.validate_python(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "pydantic_core._pydantic_core.ValidationError: 1 validation error for ChatbotDataTuples\n",
            "0\n",
            "  Tuple should have at most 2 items after validation, not 3 [type=too_long, input_value=['append', '0,1attempt to...ext.  | | | | ', 'not '], input_type=list]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/too_long\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User query for Gradio: What is the link between smoking and COVID-19 severity?\n",
            "Returning cached AI system (CondenseQuestionChatEngine).\n",
            "Querying with: What is the relationship between smoking and the risk of developing severe COVID-19, and are there any studies that have investigated this association?\n",
            "\n",
            "Note: The standalone question captures the essence of the follow-up message, which is to understand the link between smoking and COVID-19 severity. The question is concise and clear, and it does not rely on external knowledge or prior information. The question is also specific, focusing on the relationship between smoking and severe COVID-19, and it does not attempt to answer off-topic questions or provide general information outside the scope of the topic. | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://c183a7ce742b88a987.gradio.live\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "# Ensure initialize_ai_system is defined from Structure Code for UI Application.\n",
        "\n",
        "def chat_response_streaming(message, history):\n",
        "    \"\"\"\n",
        "    Handles a user's message, pre-filters for some casual inputs,\n",
        "    gets a response from the AI system for others, and streams it back.\n",
        "    \"\"\"\n",
        "    print(f\"\\nUser query for Gradio: {message}\")\n",
        "    normalized_message = message.strip().lower()\n",
        "\n",
        "    # --- Simple Pre-filter for Common Casual Inputs ---\n",
        "    if normalized_message in [\"hi\", \"hello\", \"hey\"]:\n",
        "        yield \"Hello there! I'm an AI assistant focused on COVID-19 and smoking. How can I help with your research today?\"\n",
        "        return\n",
        "\n",
        "    if normalized_message.startswith(\"my name is\"):\n",
        "        try:\n",
        "            name_part = message.split(\"my name is\", 1)[1].strip()\n",
        "            if name_part:\n",
        "                name = name_part.split(\" \")[0].capitalize()\n",
        "                yield f\"Nice to meet you, {name}! I can assist with questions about COVID-19 and smoking. What's your query?\"\n",
        "            else:\n",
        "                yield \"Okay! I'm here to help with your research on COVID-19 and smoking.\"\n",
        "            return\n",
        "        except IndexError:\n",
        "             yield \"Okay! I'm here to help with your research on COVID-19 and smoking.\"\n",
        "             return\n",
        "\n",
        "    # Handle common affirmations/closings\n",
        "    common_affirmations = [\"perfect\", \"great\", \"thanks\", \"thank you\", \"ok\", \"okay\", \"got it\", \"sounds good\", \"excellent\"]\n",
        "    if normalized_message in common_affirmations:\n",
        "        yield \"You're welcome! Is there anything else I can help you with regarding COVID-19 and smoking research?\"\n",
        "        return\n",
        "\n",
        "    # --- End of Pre-filter ---\n",
        "\n",
        "    # If not caught by pre-filters, proceed with the AI engine\n",
        "    chat_engine_instance = initialize_ai_system()\n",
        "\n",
        "    if not chat_engine_instance:\n",
        "        yield \"Error: The AI chat engine is not available. Please check the Colab notebook for errors during initialization.\"\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        response_stream = chat_engine_instance.stream_chat(message)\n",
        "        accumulated_response = \"\"\n",
        "        for token in response_stream.response_gen:\n",
        "            accumulated_response += token\n",
        "            yield accumulated_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Gradio query engine processing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        yield f\"Sorry, an error occurred while processing your request: {str(e)}\"\n",
        "\n",
        "# --- Pre-initialize the AI system before launching the UI ---\n",
        "# (This part remains the same as before)\n",
        "print(\"Pre-initializing AI system for Gradio Interface... This might take a few minutes if it's the first run in this session.\")\n",
        "engine_instance = initialize_ai_system()\n",
        "\n",
        "if engine_instance is None:\n",
        "    print(\"CRITICAL ERROR: Could not initialize AI system for Gradio. The UI cannot be launched reliably.\")\n",
        "else:\n",
        "    print(\"AI system pre-initialized successfully and is cached.\")\n",
        "\n",
        "    title = \"AI Research Assistant: CORD-19 & Smoking Linkages\"\n",
        "\n",
        "    iface = gr.ChatInterface(\n",
        "        fn=chat_response_streaming,\n",
        "        title=title,\n",
        "        description=\"Ask questions about the relationship between COVID-19 and smoking, based on CORD-19 dataset. Powered by a Llama 3 8B model.\",\n",
        "        examples=[\n",
        "            [\"What is the link between smoking and COVID-19 severity?\"],\n",
        "            [\"Does vaping affect COVID-19 outcomes?\"],\n",
        "            [\"Are smokers more susceptible to COVID-19?\"]\n",
        "        ],\n",
        "        chatbot=gr.Chatbot(height=600, label=\"Chat Conversation\"),\n",
        "        textbox=gr.Textbox(placeholder=\"Type your question here and press Enter...\", container=False, scale=7, label=\"Your Question\")\n",
        "    )\n",
        "\n",
        "    print(\"\\nLaunching Gradio Interface... Please wait for the public URL.\")\n",
        "    iface.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b0fb2def128143e19e482f04c4d79973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_404cdf68f2cf47ebbacda51542a0a822",
              "IPY_MODEL_544af5ee3b6940069d578c08d303dfb0",
              "IPY_MODEL_13bd72516dfd484e835b28548b1472b3"
            ],
            "layout": "IPY_MODEL_dbfaaa2b1ce8400d8abdd8efd1e7f541"
          }
        },
        "404cdf68f2cf47ebbacda51542a0a822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12b1d8cc0af543febb5b741ef068be96",
            "placeholder": "​",
            "style": "IPY_MODEL_33a1558d0de141a0bff4d14fc24f03d0",
            "value": "Parsing nodes: 100%"
          }
        },
        "544af5ee3b6940069d578c08d303dfb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eb9954eb52a481e85963737eaf0d659",
            "max": 5915,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c49cfb7f81b4768b32678142e536804",
            "value": 5915
          }
        },
        "13bd72516dfd484e835b28548b1472b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2079fded91324d34b835829a927d7154",
            "placeholder": "​",
            "style": "IPY_MODEL_b9e7968b463d4549893cc88a409f4c52",
            "value": " 5915/5915 [00:01&lt;00:00, 3719.21it/s]"
          }
        },
        "dbfaaa2b1ce8400d8abdd8efd1e7f541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12b1d8cc0af543febb5b741ef068be96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33a1558d0de141a0bff4d14fc24f03d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6eb9954eb52a481e85963737eaf0d659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c49cfb7f81b4768b32678142e536804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2079fded91324d34b835829a927d7154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9e7968b463d4549893cc88a409f4c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc2c4a2e23064d9393984f755c0a9f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f81ad912eca4d4ab6b644a7a576ec7d",
              "IPY_MODEL_5b18cb5f29b1458b9b137abf731d9e35",
              "IPY_MODEL_91575fcd72274a3d9a57a7cb2fe01d61"
            ],
            "layout": "IPY_MODEL_9b12c16436ad4d9b8f28ce66b1e64925"
          }
        },
        "8f81ad912eca4d4ab6b644a7a576ec7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_884e60abd4fd4ab08b781fff57fe666f",
            "placeholder": "​",
            "style": "IPY_MODEL_a4d8d4a97d174b19b1d25e0d5954e93f",
            "value": "Generating embeddings: 100%"
          }
        },
        "5b18cb5f29b1458b9b137abf731d9e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34bb86f6f6974ac4be098f96574c4b53",
            "max": 2048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd449f19f95e4e34b240e0bc134dca32",
            "value": 2048
          }
        },
        "91575fcd72274a3d9a57a7cb2fe01d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_483588898aed45d3857f16d745443630",
            "placeholder": "​",
            "style": "IPY_MODEL_abc0286f0f2a460bafcbd894e367447f",
            "value": " 2048/2048 [00:02&lt;00:00, 812.45it/s]"
          }
        },
        "9b12c16436ad4d9b8f28ce66b1e64925": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "884e60abd4fd4ab08b781fff57fe666f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4d8d4a97d174b19b1d25e0d5954e93f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34bb86f6f6974ac4be098f96574c4b53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd449f19f95e4e34b240e0bc134dca32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "483588898aed45d3857f16d745443630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abc0286f0f2a460bafcbd894e367447f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1705bd8d45c94626a95e1cc200ef14be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8429737336754a2d943b3b39b972f68e",
              "IPY_MODEL_0146439f78e140348536e27f7fbdf5ff",
              "IPY_MODEL_0324d8fd185a46cf80b774009b8b8bf3"
            ],
            "layout": "IPY_MODEL_a4349ca0e26f4c40b45bd79f43ef1e6d"
          }
        },
        "8429737336754a2d943b3b39b972f68e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76bbe6de06ff427799ff58045e750fd1",
            "placeholder": "​",
            "style": "IPY_MODEL_7bc69b7de469487d94d7758412a648c6",
            "value": "Generating embeddings: 100%"
          }
        },
        "0146439f78e140348536e27f7fbdf5ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb6b4a304cff4014957ea9931f23d6e3",
            "max": 2048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_348e90ff48974c7cbd339b99ef41911e",
            "value": 2048
          }
        },
        "0324d8fd185a46cf80b774009b8b8bf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fcb7a215f274f91b601d2feff7f0667",
            "placeholder": "​",
            "style": "IPY_MODEL_f86a4d623eca4a1bb1b3e5baafd39e76",
            "value": " 2048/2048 [00:02&lt;00:00, 827.20it/s]"
          }
        },
        "a4349ca0e26f4c40b45bd79f43ef1e6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76bbe6de06ff427799ff58045e750fd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bc69b7de469487d94d7758412a648c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb6b4a304cff4014957ea9931f23d6e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "348e90ff48974c7cbd339b99ef41911e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fcb7a215f274f91b601d2feff7f0667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f86a4d623eca4a1bb1b3e5baafd39e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36b39b08341a4298ab98854ff82fda2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6007ba6fb2b4da9a8d6ed0c13517cb9",
              "IPY_MODEL_b7e1598f057f40e08928524776efc2c6",
              "IPY_MODEL_59a6423f640c4b20bf078cc98868c9e2"
            ],
            "layout": "IPY_MODEL_db55bdebeaf14461ae7ec7f0096c289a"
          }
        },
        "f6007ba6fb2b4da9a8d6ed0c13517cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daabed5bcd9242bebc95fcc43405e3ca",
            "placeholder": "​",
            "style": "IPY_MODEL_75a4fe39b1584e48bf4781f9efb12b63",
            "value": "Generating embeddings: 100%"
          }
        },
        "b7e1598f057f40e08928524776efc2c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b52da33b810d41f0bbb7bde126d56c3e",
            "max": 1819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b98851e284414834b2e6f25f9522ec54",
            "value": 1819
          }
        },
        "59a6423f640c4b20bf078cc98868c9e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b003caa0a3b64119be84d5af2103431d",
            "placeholder": "​",
            "style": "IPY_MODEL_4175344303dd44d58b3bb99cd4b455fe",
            "value": " 1819/1819 [00:02&lt;00:00, 812.99it/s]"
          }
        },
        "db55bdebeaf14461ae7ec7f0096c289a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daabed5bcd9242bebc95fcc43405e3ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75a4fe39b1584e48bf4781f9efb12b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b52da33b810d41f0bbb7bde126d56c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b98851e284414834b2e6f25f9522ec54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b003caa0a3b64119be84d5af2103431d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4175344303dd44d58b3bb99cd4b455fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}