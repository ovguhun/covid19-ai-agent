{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhCi00Odz0OK"
      },
      "source": [
        "# COVID-19 Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK5lCEx1z4ZE"
      },
      "source": [
        "# Creating an LLM-based AI Research Agent for the CORD-19 Dataset.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The CORD-19 (COVID-19 Open Research Dataset) is a comprehensive collection of\n",
        "scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses.\n",
        "This project aims to build an AI agent that specializes in analyzing this dataset to answer questions about the relationship between COVID-19 and smoking (including cigarettes, vaping, and tobacco). The agent will leverage a Large Language Model (LLM) and Retrieval Augmented Generation (RAG) to provide insights based on the scientific literature within CORD-19."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477RASRi0vOU"
      },
      "source": [
        "# Install All Necessary Libraries\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The !pip install commands will then install all the required Python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "faCD_R-cuW3i"
      },
      "outputs": [],
      "source": [
        "# Run this cell to install all required libraries\n",
        "!pip install gradio\n",
        "!pip install llama-index llama-index-embeddings-huggingface llama-index-llms-huggingface\n",
        "!pip install pandas pyarrow tqdm\n",
        "!pip install transformers accelerate bitsandbytes torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2srLzzde1nuu"
      },
      "source": [
        "# Verify that PyTorch can see and use the CUDA-enabled GPU.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "F8RbYfGvvNX_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\") # 0 refers to the first GPU\n",
        "else:\n",
        "    print(\"WARNING: CUDA not available. Check Colab runtime settings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkJTpYBI2ErT"
      },
      "source": [
        "#Hugging Face Hub Setup\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code snippet uses the notebook_login function from the huggingface_hub library to securely connect your Colab notebook to your Hugging Face account. This authentication is necessary to download the dataset and the language model we'll be using later. You'll be prompted to enter an access token from your Hugging Face account settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fKtVqP26ve_U"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qt9IMBl2eoe"
      },
      "source": [
        "# Google Drive Integration\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code mounts your Google Drive to the Colab environment. This allows you to save important files, like the vector index we will create later, directly to your personal Google Drive. This prevents data loss if your Colab session disconnects and saves you from having to rebuild the index every time you open the notebook. You'll be asked to authorize Colab to access your Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK0JOzOrxM-r"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaVZM49h3_MI"
      },
      "source": [
        "#Download and Load the Dataset\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXjou97iyL9z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(\"Attempting to load the CORD-19 abstracts dataset from Hugging Face...\")\n",
        "\n",
        "try:\n",
        "    # This reads the dataset directly into a pandas DataFrame.\n",
        "    # It might take a moment to download.\n",
        "    df_cord19_abstracts = pd.read_parquet(\"hf://datasets/pritamdeka/cord-19-abstract/data/train-00000-of-00001.parquet\")\n",
        "    print(\"\\n✅ Successfully loaded dataset.\")\n",
        "    print(f\"The dataset has {len(df_cord19_abstracts)} abstracts.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error loading dataset from Hugging Face: {e}\")\n",
        "    print(\"Please ensure you are logged in with 'notebook_login()' and have network access.\")\n",
        "    df_cord19_abstracts = pd.DataFrame() # Create empty df to avoid later errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeyIz-hv4JSw"
      },
      "source": [
        "# Data Acquisition & Preprocessing\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Inspect and Clean Data**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code inspects the loaded data to understand its structure. .info() provides a technical summary (columns, data types), and .head() shows the first 5 rows to give us a look at the actual content. We then perform basic cleaning by removing any rows that might have an empty abstract and ensuring the abstract column is treated as text, which prevents errors in later steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVt07U-Szf3D"
      },
      "outputs": [],
      "source": [
        "# Inspect the structure of the loaded data and perform basic cleaning.\n",
        "\n",
        "# Check if the DataFrame from the previous step exists\n",
        "if 'df_cord19_abstracts' in locals() and not df_cord19_abstracts.empty:\n",
        "    print(\"--- Dataset Information ---\")\n",
        "    df_cord19_abstracts.info()\n",
        "\n",
        "    print(\"\\n\\n--- First 5 Rows of the Dataset ---\")\n",
        "    # Using display() in Colab provides a nicer table format\n",
        "    display(df_cord19_abstracts.head())\n",
        "\n",
        "    # --- Data Cleaning ---\n",
        "    print(\"\\n\\n--- Cleaning Data ---\")\n",
        "    # Remove rows where the 'abstract' column is empty or missing\n",
        "    df_cord19_abstracts.dropna(subset=['abstract'], inplace=True)\n",
        "    # Ensure the abstract column is treated as the 'string' data type\n",
        "    df_cord19_abstracts['abstract'] = df_cord19_abstracts['abstract'].astype(str)\n",
        "\n",
        "    print(\"✅ Data cleaned: Removed empty abstracts and ensured text format.\")\n",
        "    print(f\"The dataset now has {len(df_cord19_abstracts)} abstracts after cleaning.\")\n",
        "else:\n",
        "    print(\"❌ DataFrame 'df_cord19_abstracts' was not loaded correctly in the previous step. Please check if you Load the Dataset successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYUnUZcw5TUN"
      },
      "source": [
        "**Keyword-based Filtering**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code snippet filters our large DataFrame of abstracts to create a smaller, more focused one. First, it defines a keywords list containing terms related to smoking. It then uses pandas' powerful str.contains() function to search the 'abstract' column in a case-insensitive way for any of these keywords. The result is a new DataFrame, df_relevant_abstracts, containing only the documents relevant to our research question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXhqZayk0EAm"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to include only abstracts containing specific keywords.\n",
        "\n",
        "# First, check if the main DataFrame from the previous steps exists\n",
        "if 'df_cord19_abstracts' in locals() and not df_cord19_abstracts.empty:\n",
        "    print(\"--- Filtering Abstracts by Keywords ---\")\n",
        "\n",
        "    # Define your keywords related to smoking\n",
        "    keywords = [\"smoking\", \"cigarette\", \"nicotine\", \"vaping\", \"tobacco\", \"e-cigarette\", \"smoker\"]\n",
        "    print(f\"Keywords for filtering: {keywords}\")\n",
        "\n",
        "    # Create a search pattern: \"keyword1|keyword2|keyword3\" which means \"keyword1 OR keyword2 OR keyword3\"\n",
        "    search_terms_pattern = '|'.join(keywords)\n",
        "\n",
        "    # Filter the DataFrame and create a new one with only relevant abstracts\n",
        "    df_relevant_abstracts = df_cord19_abstracts[df_cord19_abstracts['abstract'].str.contains(search_terms_pattern, case=False, na=False)]\n",
        "\n",
        "    print(f\"\\n✅ Found {len(df_relevant_abstracts)} relevant abstracts after keyword filtering.\")\n",
        "\n",
        "    # Display the first few relevant abstracts to confirm the filtering worked\n",
        "    if len(df_relevant_abstracts) > 0:\n",
        "        print(\"\\n--- First 5 Relevant Abstracts ---\")\n",
        "        display(df_relevant_abstracts.head())\n",
        "    else:\n",
        "        print(\"\\n⚠️ Warning: No relevant abstracts were found for the given keywords.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ DataFrame 'df_cord19_abstracts' was not found or is empty. Please run the previous steps first.\")\n",
        "    # Create an empty DataFrame to prevent errors in later cells\n",
        "    df_relevant_abstracts = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTb_jJ_T8CKd"
      },
      "source": [
        "# Vector Database Creation & Management\n",
        "\n",
        "**Configure LlamaIndex Settings**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code snippet configures LlamaIndex's global settings. It specifies which model to use for creating vector embeddings (sentence-transformers/all-MiniLM-L6-v2) and explicitly tells LlamaIndex to use the GPU (cuda) for this process, which significantly speeds it up. The Settings.llm is set to None for now, as we only need the embedding model in this phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1buq6HCw2Wjs"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "print(\"--- Configuring LlamaIndex Settings for Embedding Model ---\")\n",
        "\n",
        "# Set the embedding model to be used for converting text to vectors.\n",
        "# 'all-MiniLM-L6-v2' is a popular and efficient model for this.\n",
        "# 'device=\"cuda\"' ensures the GPU is used for this computationally intensive task.\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    device=\"cuda\"\n",
        ")\n",
        "\n",
        "# We are not using a Large Language Model (LLM) in this phase,\n",
        "# so we set it to None in the global settings for now.\n",
        "Settings.llm = None\n",
        "\n",
        "print(\"\\n✅ Embedding model configured to run on CUDA.\")\n",
        "print(\"   LlamaIndex will now use 'all-MiniLM-L6-v2' for creating text embeddings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmWZr9XA8nuo"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Prepare Documents and Chunking**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code takes the filtered abstracts (from df_relevant_abstracts) and breaks each one down into smaller, 150-word \"chunks.\" Each chunk is then converted into a LlamaIndex Document object. This chunking process is important because it helps the AI pinpoint very specific pieces of information within the larger abstracts when searching for answers. A progress bar (tqdm) will show the status as it processes the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNObWnNK2kYE"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "from tqdm import tqdm # For displaying a progress bar\n",
        "\n",
        "# This list will hold all our chunked Document objects\n",
        "documents = []\n",
        "\n",
        "# First, check if the DataFrame with relevant abstracts exists and is not empty\n",
        "if 'df_relevant_abstracts' in locals() and not df_relevant_abstracts.empty:\n",
        "    print(f\"--- Preparing and chunking {len(df_relevant_abstracts)} relevant abstracts ---\")\n",
        "\n",
        "    # Get the list of abstract texts to process from the 'abstract' column\n",
        "    texts_to_process = df_relevant_abstracts['abstract'].tolist()\n",
        "\n",
        "    # Define the desired chunk size in words\n",
        "    chunk_size_by_words = 150\n",
        "\n",
        "    # Loop through each abstract, split it into words, and create chunks\n",
        "    for text in tqdm(texts_to_process, desc=\"Chunking abstracts\"):\n",
        "        words = text.split() # Split the abstract into a list of words\n",
        "        # Iterate through the words list, taking 'chunk_size_by_words' at a time\n",
        "        for i in range(0, len(words), chunk_size_by_words):\n",
        "            # Join the words in the current chunk back into a string\n",
        "            chunk_text = \" \".join(words[i:i + chunk_size_by_words]).strip()\n",
        "            # Create a LlamaIndex Document object if the chunk is not empty\n",
        "            if chunk_text:\n",
        "                documents.append(Document(text=chunk_text))\n",
        "\n",
        "    print(f\"\\n✅ Created {len(documents)} document chunks from the relevant abstracts.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ No relevant abstracts found ('df_relevant_abstracts' is missing or empty).\")\n",
        "    print(\"   Please ensure previously (Keyword-based Filtering) ran successfully and found some abstracts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNSZkBtY9JsE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Create and Persist VectorStoreIndex**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code snippet takes the 5915 document chunks we just created and uses the configured embedding model (all-MiniLM-L6-v2 on the GPU) to convert each chunk into a numerical vector. These vectors are then stored in a VectorStoreIndex, which is an optimized data structure that allows for very fast similarity searches. The code also \"persists\" (saves) this index to a specified directory (either on your Google Drive if mounted, or in Colab's temporary storage) so it can be reloaded later without rebuilding. This step will take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca1E5vLT20Qk"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "import os # To check for Google Drive path and create directories\n",
        "\n",
        "print(\"--- Creating and Persisting VectorStoreIndex ---\")\n",
        "\n",
        "# Initialize the index variable\n",
        "index = None\n",
        "\n",
        "# --- Define the directory where the index will be saved ---\n",
        "# Option 1: Google Drive (Recommended for persistence)\n",
        "# IMPORTANT: Replace 'YourProjectFolderOnDrive' with an actual folder name you want in your Google Drive.\n",
        "# This folder will be created if it doesn't exist.\n",
        "drive_persist_dir = \"/content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\"\n",
        "\n",
        "# Option 2: Local Colab storage (index will be lost if Colab session ends)\n",
        "colab_local_persist_dir = \"storage_cord19_smoking_index_colab\"\n",
        "\n",
        "# Determine the persist_dir based on whether Google Drive is mounted\n",
        "persist_dir = \"\"\n",
        "if os.path.exists(\"/content/drive/MyDrive/\"): # Check if the base MyDrive folder exists\n",
        "    persist_dir = drive_persist_dir\n",
        "    print(f\"Google Drive detected. Index will be saved to: {persist_dir}\")\n",
        "else:\n",
        "    persist_dir = colab_local_persist_dir\n",
        "    print(f\"Google Drive not detected or not accessible at '/content/drive/MyDrive/'.\")\n",
        "    print(f\"Index will be saved to local Colab storage: {persist_dir}\")\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "# This is important for both Google Drive and local Colab storage.\n",
        "try:\n",
        "    os.makedirs(persist_dir, exist_ok=True)\n",
        "    print(f\"Ensured directory exists: {persist_dir}\")\n",
        "except OSError as e:\n",
        "    print(f\"Error creating directory {persist_dir}: {e}. Please check the path and permissions.\")\n",
        "    # If directory creation fails, we should not proceed with saving.\n",
        "    # For now, we'll let the next step potentially fail if 'documents' is empty,\n",
        "    # but a more robust solution might stop here.\n",
        "\n",
        "\n",
        "# Check if the 'documents' list exists and has content\n",
        "if 'documents' in locals() and documents:\n",
        "    print(f\"\\nCreating vector index from {len(documents)} document chunks...\")\n",
        "    print(\"This process will use the GPU and may take 5-15 minutes. Please be patient.\")\n",
        "\n",
        "    # This is the core step: generate embeddings and build the index.\n",
        "    # LlamaIndex will use the 'Settings.embed_model' we configured previously.\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents,\n",
        "        show_progress=True # Displays a progress bar\n",
        "    )\n",
        "\n",
        "    # Save the created index to the specified 'persist_dir'\n",
        "    index.storage_context.persist(persist_dir=persist_dir)\n",
        "    print(f\"\\n✅ VectorStoreIndex created and successfully saved to: {persist_dir}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n❌ No document chunks found ('documents' list is missing or empty).\")\n",
        "    print(\"   Cannot create the VectorStoreIndex. Please ensure 'Prepare Documents and Chunking' ran successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ERnMr9t9134"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Load Index from Storage**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code snippet checks if a previously saved vector index exists in the specified directory (persist_dir). If the index was just created in the current session, it won't try to reload it. However, if this is a new Colab session and the index object doesn't exist yet, this code will load the index from disk (from your Google Drive or local Colab storage, depending on where it was saved). This avoids the time-consuming process of rebuilding the index every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slK4gXh63_vk"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "import os\n",
        "\n",
        "print(\"--- Attempting to Load Index from Storage ---\")\n",
        "\n",
        "# --- Ensure 'persist_dir' is defined and matches the save location previously ---\n",
        "# The 'persist_dir' variable should still be in memory from the previous cell.\n",
        "# If you are running this cell in a new session, you might need to redefine 'persist_dir'\n",
        "# to point to where your index was saved. For example:\n",
        "# persist_dir = \"/content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\"\n",
        "# OR\n",
        "# persist_dir = \"storage_cord19_smoking_index_colab\"\n",
        "\n",
        "# Check if 'persist_dir' is defined. If not, it means previous snippet was likely not run in this session.\n",
        "if 'persist_dir' not in locals():\n",
        "    print(\"❌ 'persist_dir' is not defined. This usually means previous snippet (saving the index) was not run in this session.\")\n",
        "    print(\"   Please define 'persist_dir' to point to your saved index location or run previous to create it.\")\n",
        "    # Attempt to set a default if drive is mounted, otherwise local. This is a fallback.\n",
        "    if os.path.exists(\"/content/drive/MyDrive/\"):\n",
        "        persist_dir = \"/content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\" # Default drive path\n",
        "        print(f\"   Attempting to use default Google Drive path: {persist_dir}\")\n",
        "    else:\n",
        "        persist_dir = \"storage_cord19_smoking_index_colab\" # Default local path\n",
        "        print(f\"   Attempting to use default local Colab path: {persist_dir}\")\n",
        "\n",
        "print(f\"Checking for existing index in: {persist_dir}\")\n",
        "\n",
        "# We try to load the index if:\n",
        "# 1. The 'index' variable doesn't already exist OR it exists but is None (meaning it wasn't successfully created/loaded yet).\n",
        "# 2. The 'persist_dir' (the directory where the index should be saved) actually exists.\n",
        "if ('index' not in locals() or index is None) and os.path.exists(persist_dir):\n",
        "    print(f\"Found existing index directory. Attempting to load index...\")\n",
        "    try:\n",
        "        # Prepare the storage context pointing to the directory.\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
        "        # Load the index. This will re-populate the 'index' variable.\n",
        "        index = load_index_from_storage(storage_context)\n",
        "        print(\"✅ Index loaded successfully from storage.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading index from storage: {e}\")\n",
        "        print(\"   The saved index might be corrupted, or the path might be incorrect.\")\n",
        "        print(\"   You might need to rebuild the index by re-running previous snippet.\")\n",
        "        index = None # Ensure index is None if loading failed\n",
        "\n",
        "elif 'index' in locals() and index is not None:\n",
        "    # This case means the index was already created or loaded in the current session (e.g., by running previous snippet).\n",
        "    print(\"ℹ️ Index object already exists in this session (likely created in previous snippet). No need to reload.\")\n",
        "\n",
        "else:\n",
        "    # This case means 'persist_dir' does not exist, and 'index' is not already populated.\n",
        "    print(f\"ℹ️ No existing index found at {persist_dir}.\")\n",
        "    print(\"   If this is your first time running through all steps, this is normal (index was just created in previous code snippet).\")\n",
        "    print(\"   If you expected to load an index from a previous session, ensure 'persist_dir' is correct\")\n",
        "    print(\"   and that you successfully saved the index in that previous sessio.\")\n",
        "\n",
        "# Final check to see if the 'index' object is now available for use\n",
        "if 'index' in locals() and index is not None:\n",
        "    print(\"\\n👍 Index object is available and ready for use.\")\n",
        "else:\n",
        "    print(\"\\n⚠️ Index object is NOT available. Subsequent steps requiring the index may fail.\")\n",
        "    print(\"   Please review the messages above to diagnose the issue.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxQ1xZ-iIPVT"
      },
      "source": [
        "# AI Agent Development\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Configure the Large Language Model (LLM)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code snippet configures and loads the specific Large Language Model (LLM) that our agent will use as its \"brain.\" We're using unsloth/llama-3-8b-Instruct-bnb-4bit, a 4-bit quantized version of Llama 3 8B, which offers a good balance of performance (speed) and quality. The code specifies the model name, tokenizer, context window size, maximum new tokens to generate, and ensures it runs on the GPU using 16-bit precision for efficiency. This configured LLM is then set as the global default for LlamaIndex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsbpo_On4uIW"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "import torch # For torch_dtype\n",
        "\n",
        "print(\"--- Configuring the Large Language Model (LLM) ---\")\n",
        "\n",
        "# Configure and load the Unsloth Quantized Llama 3 8B model\n",
        "try:\n",
        "    llm = HuggingFaceLLM(\n",
        "        model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "        tokenizer_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "        context_window=2048, # Max tokens the model can see in total\n",
        "        max_new_tokens=256,  # Max tokens the model will generate in one answer\n",
        "        device_map=\"auto\",   # Automatically use the GPU\n",
        "        model_kwargs={\"torch_dtype\": torch.float16}, # Optimized for Unsloth models\n",
        "        generate_kwargs={\"temperature\": 0.7, \"do_sample\": True} # Controls response creativity\n",
        "    )\n",
        "    Settings.llm = llm # Set this as the global LLM for LlamaIndex\n",
        "    print(\"\\n✅ LLM (unsloth/llama-3-8b-Instruct-bnb-4bit) configured successfully.\")\n",
        "    print(\"   It's set as the default LLM in LlamaIndex Settings and will run on the GPU.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ CRITICAL ERROR: Failed to configure LLM. Error: {e}\")\n",
        "    print(\"   Troubleshooting suggestions:\")\n",
        "    print(\"     - Ensure you are logged into Hugging Face.\")\n",
        "    print(\"     - Double-check the model name for typos.\")\n",
        "    print(\"     - Ensure your Colab environment has the A100 GPU selected and enough resources.\")\n",
        "    Settings.llm = None # Ensure LLM is None if setup fails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dFD72IcLri7"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Assemble the RAG Query Engine**\n",
        "\n",
        "---\n",
        "This code snippet assembles the core components of our Retrieval Augmented Generation (RAG) system.\n",
        "\n",
        "1. Retriever: It creates a retriever from our previously loaded vector index (index), configured to fetch the top 7 most relevant document chunks (similarity_top_k=7) for any query.\n",
        "2. Prompt Template: It defines a specific set of instructions (qa_prompt_template_str) telling the LLM how to behave: act as a research assistant, use only the provided CORD-19 context, synthesize information, and be clear.\n",
        "3. Response Synthesizer: This component takes the retrieved chunks, the user's query, and the prompt template, and uses the configured LLM to generate the final textual answer. We enable streaming=True here for a better user experience later in the UI.\n",
        "4. Query Engine: Finally, it combines the retriever and response synthesizer into a RetrieverQueryEngine, which is our complete system for answering questions based on the CORD-19 data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NMNyUw749Em"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings, PromptTemplate\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
        "# 'os' might not be strictly needed here if 'index' and 'Settings.llm' are guaranteed to exist\n",
        "# from previous steps, but it's harmless.\n",
        "\n",
        "print(\"--- Creating the Base RAG Query Engine ---\")\n",
        "\n",
        "# Initialize the engine variable\n",
        "base_query_engine = None\n",
        "\n",
        "# Ensure the index is loaded and Settings.llm is configured from previous steps\n",
        "if 'index' in locals() and index is not None and Settings.llm is not None:\n",
        "\n",
        "    # --- 1. Create the Retriever ---\n",
        "    print(\"Creating retriever from the index...\")\n",
        "    # Retrieve the top 7 most similar chunks for a balance of speed and context.\n",
        "    retriever = index.as_retriever(similarity_top_k=7)\n",
        "    print(f\"✅ Retriever configured to fetch top {retriever.similarity_top_k} chunks.\")\n",
        "\n",
        "    # --- 2. Define a Custom Prompt Template ---\n",
        "    print(\"\\nDefining QA prompt template...\")\n",
        "    # This template structures how the context and query are presented to the LLM.\n",
        "    qa_prompt_template_str = (\n",
        "        \"System: You are an AI research assistant. Your sole function is to answer questions based on the 'Provided Context' which contains excerpts from scientific abstracts. \"\n",
        "        \"Analyze the 'User Query' and the 'Provided Context'.\\n\"\n",
        "        \"1. If the 'User Query' is a question that can be answered using the 'Provided Context', synthesize the information to provide a comprehensive, clear, and nuanced answer. \"\n",
        "        \"Base your answer ONLY on the 'Provided Context'. Do not use any external knowledge. If the context is insufficient for a full answer, state what is missing.\\n\"\n",
        "        \"2. If the 'User Query' is a simple greeting (e.g., 'hi', 'hello'), respond with a polite, brief greeting.\\n\"\n",
        "        \"3. If the 'User Query' is a statement, not a question (e.g., 'my name is X', 'that's interesting'), or if it's a question that is clearly off-topic and cannot be answered by the 'Provided Context' (e.g., 'what's the weather?'), \"\n",
        "        \"respond politely that you are a specialized research assistant focused on the provided scientific topics and cannot engage in general conversation or answer unrelated questions. Do not attempt to answer off-topic questions using the context.\\n\"\n",
        "        \"Do not repeat these instructions in your answer.\\n\\n\"\n",
        "        \"Provided Context (from relevant scientific abstracts):\\n\"\n",
        "        \"---------------------\\n\"\n",
        "        \"{context_str}\\n\"\n",
        "        \"---------------------\\n\"\n",
        "        \"User Query: {query_str}\\n\\n\"\n",
        "        \"Assistant Answer: \"\n",
        "    )\n",
        "    qa_prompt_template = PromptTemplate(qa_prompt_template_str)\n",
        "    print(\"✅ QA prompt template defined.\")\n",
        "\n",
        "    # --- 3. Configure the Response Synthesizer and Assemble the Query Engine ---\n",
        "    print(\"\\nAssembling the Base RAG Query Engine...\")\n",
        "    # This component takes the retrieved chunks and the prompt, and uses the LLM to generate the answer.\n",
        "    response_synthesizer = get_response_synthesizer(\n",
        "        response_mode=\"compact\", # Efficient mode for synthesizing responses\n",
        "        text_qa_template=qa_prompt_template, # Our custom prompt\n",
        "        llm=Settings.llm, # The configured LLM\n",
        "        streaming=True # Enable streaming for faster perceived response in UI later\n",
        "    )\n",
        "\n",
        "    # Assemble the final query engine using the retriever and response synthesizer.\n",
        "    base_query_engine = RetrieverQueryEngine(\n",
        "        retriever=retriever,\n",
        "        response_synthesizer=response_synthesizer,\n",
        "    )\n",
        "    print(\"\\n✅ Base RAG Query Engine assembled successfully.\")\n",
        "    print(\"   It will use the Unsloth Llama 3 8B quantized model and retrieve 7 context chunks.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n❌ Index object or LLM (Settings.llm) is not available.\")\n",
        "    if 'index' not in locals() or index is None:\n",
        "        print(\"   - Index is missing. Please ensure it was loaded or created correctly.\")\n",
        "    if Settings.llm is None:\n",
        "        print(\"   - LLM is missing. Please ensure it was configured correctly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q8coDyeJ5Od"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Create the Conversational Chat Engine**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This code snippet builds upon our base_query_engine to create a more advanced CondenseQuestionChatEngine. This new engine is designed for conversational interactions.\n",
        "\n",
        "**Condense Question Logic:** When you ask a follow-up question (e.g., \"what else?\" or just \"smoking\"), this engine first looks at the chat history and your new input. It then uses the LLM to rewrite your input into a complete, standalone question that makes sense given the conversation so far.\n",
        "\n",
        "**Uses Base Engine:** This newly formulated standalone question is then passed to our base_query_engine (which is excellent at answering specific, well-formed questions using the CORD-19 data). This approach allows the agent to handle vague follow-ups and maintain conversational context effectively. The `verbose=False` setting means it won't print the condensed questions during operation, keeping the output clean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqYFVocKk1ln"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
        "# Settings, PromptTemplate, RetrieverQueryEngine, get_response_synthesizer should be available\n",
        "# if previous cells were run, but it's good practice to ensure necessary imports are covered\n",
        "# if a cell is meant to be potentially runnable in isolation after kernel restarts.\n",
        "# However, for this step-by-step, we assume 'base_query_engine' exists.\n",
        "\n",
        "print(\"--- Creating the Conversational Chat Engine ---\")\n",
        "\n",
        "# Initialize the conversational chat engine variable\n",
        "conversational_chat_engine = None\n",
        "\n",
        "# Ensure the base_query_engine (from Assemble the RAG Query Engine) and Settings.llm are available\n",
        "if 'base_query_engine' in locals() and base_query_engine is not None and Settings.llm is not None:\n",
        "\n",
        "    try:\n",
        "        # Create the CondenseQuestionChatEngine.\n",
        "        # This engine uses the base_query_engine to answer the rephrased (condensed) question.\n",
        "        # It manages chat history internally to understand follow-up questions.\n",
        "        conversational_chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
        "            query_engine=base_query_engine, # The engine we built in Assemble the RAG Query Engine\n",
        "            # We can customize the condense_prompt if needed, but defaults are often good.\n",
        "            # For example, to see the condensed questions, you can set verbose=True\n",
        "            verbose=False\n",
        "        )\n",
        "        print(\"\\n✅ Conversational Chat Engine (CondenseQuestionChatEngine) created successfully.\")\n",
        "        print(\"   It will use the base RAG query engine to answer questions after rephrasing them based on chat history.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ CRITICAL ERROR: Failed to create CondenseQuestionChatEngine. Error: {e}\")\n",
        "        print(\"   Ensure 'base_query_engine' was created successfully in the previous step.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n❌ Base Query Engine ('base_query_engine') or LLM (Settings.llm) is not available.\")\n",
        "    if 'base_query_engine' not in locals() or base_query_engine is None:\n",
        "        print(\"   - 'base_query_engine' is missing. Please ensure Assemble the RAG Query Engine was completed successfully.\")\n",
        "    if Settings.llm is None: # Should have been caught in Assemble the RAG Query Engine, but good to check\n",
        "        print(\"   - LLM is missing. Please ensure Configuring the Large Language Model (LLM) was successful.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zl9swEzQKHk"
      },
      "source": [
        "#Application Development (Gradio GUI)\n",
        "\n",
        "---\n",
        "**Structure Code for UI Application**\n",
        "\n",
        "---\n",
        "\n",
        "This code defines a single, crucial function called initialize_ai_system(). This function encapsulates all the setup steps required to get our AI agent ready:\n",
        "\n",
        "1. Configuring the embedding model (from Configure LlamaIndex Settings).\n",
        "2. Loading our saved vector index (from Load Index from Storage).\n",
        "3. Configuring the Unsloth Llama 3 8B quantized LLM (from Configure the Large Language Model (LLM)).\n",
        "4. Assembling the complete RAG query engine (retriever, prompt template, response synthesizer - from  Assemble the RAG Query Engine). It also includes a simple caching mechanism (CACHED_QUERY_ENGINE) so that this entire setup process only runs once per Colab session, making subsequent uses of the agent much faster. This function is essential for a clean and efficient Gradio application.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NLmI4NM6n4i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch # For torch.float16\n",
        "import gradio as gr\n",
        "from llama_index.core import (\n",
        "    Settings,\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        "    PromptTemplate\n",
        ")\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
        "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "# --- Global Cache for the Chat Engine ---\n",
        "CACHED_CHAT_ENGINE = None\n",
        "\n",
        "def initialize_ai_system():\n",
        "    \"\"\"\n",
        "    Initializes all AI components (embedding model, index, LLM,\n",
        "    base query engine, and then the CondenseQuestionChatEngine with a custom condense prompt).\n",
        "    Caches and returns the CondenseQuestionChatEngine.\n",
        "    \"\"\"\n",
        "    global CACHED_CHAT_ENGINE\n",
        "\n",
        "    if CACHED_CHAT_ENGINE is not None:\n",
        "        print(\"Returning cached AI system (CondenseQuestionChatEngine).\")\n",
        "        return CACHED_CHAT_ENGINE\n",
        "\n",
        "    print(\"--- Initializing AI System with CondenseQuestionChatEngine (Custom Condense Prompt) ---\")\n",
        "\n",
        "    # --- 1. CONFIGURE EMBEDDING MODEL ---\n",
        "    print(\"Configuring embedding model...\")\n",
        "    try:\n",
        "        Settings.embed_model = HuggingFaceEmbedding(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            device=\"cuda\"\n",
        "        )\n",
        "        print(\"✅ Embedding model configured.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR configuring embedding model: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 2. LOAD THE VECTOR INDEX ---\n",
        "    print(\"\\nLoading vector index...\")\n",
        "    persist_dir = \"\"\n",
        "    drive_path = \"/content/drive/MyDrive/CORD19_Smoking_Chatbot_Index\"\n",
        "    colab_local_path = \"storage_cord19_smoking_index_colab\"\n",
        "    if os.path.exists(drive_path):\n",
        "        persist_dir = drive_path\n",
        "        print(f\"   Attempting to load index from Google Drive: {persist_dir}\")\n",
        "    elif os.path.exists(colab_local_path):\n",
        "        persist_dir = colab_local_path\n",
        "        print(f\"   Attempting to load index from local Colab storage: {persist_dir}\")\n",
        "    else:\n",
        "        print(f\"❌ CRITICAL ERROR: Index directory not found at expected Google Drive path ('{drive_path}') or local Colab path ('{colab_local_path}').\")\n",
        "        return None\n",
        "\n",
        "    if not os.path.exists(persist_dir):\n",
        "        print(f\"❌ CRITICAL ERROR: Selected persist_dir ('{persist_dir}') does not exist. Cannot load index.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
        "        index = load_index_from_storage(storage_context)\n",
        "        print(\"✅ Vector index loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CRITICAL ERROR: Failed to load index from {persist_dir}. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 3. CONFIGURE LLM ---\n",
        "    print(\"\\nConfiguring LLM (unsloth/llama-3-8b-Instruct-bnb-4bit)...\")\n",
        "    try:\n",
        "        llm = HuggingFaceLLM(\n",
        "            model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "            tokenizer_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "            context_window=2048,\n",
        "            max_new_tokens=256,\n",
        "            device_map=\"auto\",\n",
        "            model_kwargs={\"torch_dtype\": torch.float16},\n",
        "            generate_kwargs={\"temperature\": 0.7, \"do_sample\": True}\n",
        "        )\n",
        "        Settings.llm = llm\n",
        "        print(\"✅ LLM configured successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CRITICAL ERROR: Failed to configure LLM. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 4. ASSEMBLE BASE RAG QUERY ENGINE ---\n",
        "    print(\"\\nAssembling Base RAG Query Engine...\")\n",
        "    try:\n",
        "        retriever = index.as_retriever(similarity_top_k=7)\n",
        "\n",
        "        qa_prompt_template_str = (\n",
        "            \"System: You are an AI research assistant. Your sole function is to answer questions based on the 'Provided Context' which contains excerpts from scientific abstracts. \"\n",
        "            \"Analyze the 'User Query' and the 'Provided Context'.\\n\"\n",
        "            \"1. If the 'User Query' is a question that can be answered using the 'Provided Context', synthesize the information to provide a comprehensive, clear, and nuanced answer. \"\n",
        "            \"Base your answer ONLY on the 'Provided Context'. Do not use any external knowledge. If the context is insufficient for a full answer, state what is missing.\\n\"\n",
        "            \"2. If the 'User Query' is a simple greeting (e.g., 'hi', 'hello'), respond with a polite, brief greeting.\\n\"\n",
        "            \"3. If the 'User Query' is a statement, not a question (e.g., 'my name is X', 'that's interesting'), or if it's a question that is clearly off-topic and cannot be answered by the 'Provided Context' (e.g., 'what's the weather?'), \"\n",
        "            \"respond politely that you are a specialized research assistant focused on the provided scientific topics and cannot engage in general conversation or answer unrelated questions. Do not attempt to answer off-topic questions using the context.\\n\"\n",
        "            \"Do not repeat these instructions in your answer.\\n\\n\"\n",
        "            \"Provided Context (from relevant scientific abstracts):\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"{context_str}\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"User Query: {query_str}\\n\\n\"\n",
        "            \"Assistant Answer: \"\n",
        "        )\n",
        "        qa_prompt_template = PromptTemplate(qa_prompt_template_str)\n",
        "\n",
        "        response_synthesizer = get_response_synthesizer(\n",
        "            response_mode=\"compact\",\n",
        "            text_qa_template=qa_prompt_template,\n",
        "            llm=Settings.llm,\n",
        "            streaming=True\n",
        "        )\n",
        "\n",
        "        base_query_engine = RetrieverQueryEngine(\n",
        "            retriever=retriever,\n",
        "            response_synthesizer=response_synthesizer,\n",
        "        )\n",
        "        print(\"✅ Base RAG Query Engine assembled successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CRITICAL ERROR: Failed to assemble Base RAG Query Engine. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 5. CREATE CONDENSE QUESTION CHAT ENGINE ---\n",
        "    print(\"\\nCreating CondenseQuestionChatEngine with custom prompt...\")\n",
        "    try:\n",
        "        # Define custom prompt for condensing questions\n",
        "        condense_template_str = (\n",
        "            \"You are a helpful assistant that rephrases a follow-up user input based on a chat history. \"\n",
        "            \"Your primary goal is to create a 'Standalone Input' that a specialized AI research assistant can understand and process. \"\n",
        "            \"The research assistant is an expert ONLY on COVID-19 and smoking, using a specific dataset of scientific abstracts.\\n\\n\"\n",
        "            \"Carefully analyze the 'Follow Up Input' in the context of the 'Chat History'.\\n\"\n",
        "            \"1. If the 'Follow Up Input' is a question clearly seeking more information or clarification related to the 'Chat History' about COVID-19/smoking (e.g., 'what else?', 'tell me more about that specific finding', 'can you elaborate on the odds ratio?'), \"\n",
        "            \"rephrase it into a detailed, standalone question that incorporates the necessary context from the Chat History for the research AI.\\n\"\n",
        "            \"2. If the 'Follow Up Input' is a general term central to the research AI's expertise (e.g., 'smoking', 'vaping', 'nicotine and covid'), \"\n",
        "            \"rephrase it as a specific question asking for a summary of its relationship with COVID-19 based on the scientific abstracts (e.g., 'What is the relationship between smoking and COVID-19 according to the abstracts?').\\n\"\n",
        "            \"3. If the 'Follow Up Input' is clearly a simple greeting (e.g., 'hi', 'hello'), a personal statement (e.g., 'my name is Ayse', 'I am a doctor'), or a question completely unrelated to COVID-19/smoking (e.g., 'what's the weather?', 'tell me a joke'), \"\n",
        "            \"then the 'Standalone Input' should be EXACTLY the same as the 'Follow Up Input' without any modification or rephrasing.\\n\\n\"\n",
        "            \"Chat History (summarized if long):\\n\" # Added a note about summary for long history\n",
        "            \"{chat_history}\\n\\n\"\n",
        "            \"Follow Up Input: {question}\\n\\n\"\n",
        "            \"Standalone Input: \"\n",
        "        )\n",
        "        custom_condense_prompt = PromptTemplate(condense_template_str)\n",
        "\n",
        "        condense_chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
        "            query_engine=base_query_engine,\n",
        "            condense_template_prompt=custom_condense_prompt, # Use the NEW custom prompt\n",
        "            verbose=True # <<--- Set verbose=True here to see the condensed question!\n",
        "        )\n",
        "        print(\"✅ CondenseQuestionChatEngine created successfully with custom condense prompt.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CRITICAL ERROR: Failed to create CondenseQuestionChatEngine. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 6. CACHE AND RETURN THE CHAT ENGINE ---\n",
        "    print(\"\\nCaching the CondenseQuestionChatEngine.\")\n",
        "    CACHED_CHAT_ENGINE = condense_chat_engine\n",
        "    print(\"\\n✅ AI System Initialized with smarter CondenseQuestionChatEngine and Cached Successfully.\")\n",
        "    return CACHED_CHAT_ENGINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wcmxDIIQy9o"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "**Develop the Gradio Application**\n",
        "\n",
        "---\n",
        "\n",
        "This code snippet builds and launches your interactive web UI using Gradio.\n",
        "\n",
        "\n",
        "\n",
        "1.   **chat_response(message, history) function:** This function is called by Gradio every time a user sends a message.\n",
        "\n",
        "        *   It first calls our `initialize_ai_system()` to get the (potentially cached) query engine.\n",
        "        *   It then sends the user's `message` to this engine.\n",
        "        *   Crucially, it iterates through the `response_stream.response_gen` to `yield` parts of the answer as they are generated by the LLM. This creates a true **streaming effect** in the Gradio UI, making it feel much more responsive as the user sees words appear almost immediately.\n",
        "\n",
        "2.   **Pre-initialization:** Before launching the UI, we explicitly call `initialize_ai_system()`. This ensures that if it's the first run, the potentially slow setup (**model loading, etc.**) happens before the UI link is generated, preventing a timeout or a very slow first interaction for the user.\n",
        "\n",
        "3.   **gr.ChatInterface:** This Gradio component quickly creates a full chatbot UI. We tell it to use our `chat_response` function.\n",
        "4.   **iface.launch(share=True, debug=True):** This launches the web server for the UI. `share=True` generates a public URL that you can open in your browser (and share with others for ~72 hours). `debug=True` will show any Gradio-specific errors in the Colab output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ukL26xxiHdA7"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "# Ensure initialize_ai_system is defined from Structure Code for UI Application.\n",
        "\n",
        "def chat_response_streaming(message, history):\n",
        "    \"\"\"\n",
        "    Handles a user's message, pre-filters for some casual inputs,\n",
        "    gets a response from the AI system for others, and streams it back.\n",
        "    \"\"\"\n",
        "    print(f\"\\nUser query for Gradio: {message}\")\n",
        "    normalized_message = message.strip().lower()\n",
        "\n",
        "    # --- Simple Pre-filter for Common Casual Inputs ---\n",
        "    if normalized_message in [\"hi\", \"hello\", \"hey\"]:\n",
        "        yield \"Hello there! I'm an AI assistant focused on COVID-19 and smoking. How can I help with your research today?\"\n",
        "        return\n",
        "\n",
        "    if normalized_message.startswith(\"my name is\"):\n",
        "        try:\n",
        "            name_part = message.split(\"my name is\", 1)[1].strip()\n",
        "            if name_part:\n",
        "                name = name_part.split(\" \")[0].capitalize()\n",
        "                yield f\"Nice to meet you, {name}! I can assist with questions about COVID-19 and smoking. What's your query?\"\n",
        "            else:\n",
        "                yield \"Okay! I'm here to help with your research on COVID-19 and smoking.\"\n",
        "            return\n",
        "        except IndexError:\n",
        "             yield \"Okay! I'm here to help with your research on COVID-19 and smoking.\"\n",
        "             return\n",
        "\n",
        "    # Handle common affirmations/closings\n",
        "    common_affirmations = [\"perfect\", \"great\", \"thanks\", \"thank you\", \"ok\", \"okay\", \"got it\", \"sounds good\", \"excellent\"]\n",
        "    if normalized_message in common_affirmations:\n",
        "        yield \"You're welcome! Is there anything else I can help you with regarding COVID-19 and smoking research?\"\n",
        "        return\n",
        "\n",
        "    # --- End of Pre-filter ---\n",
        "\n",
        "    # If not caught by pre-filters, proceed with the AI engine\n",
        "    chat_engine_instance = initialize_ai_system()\n",
        "\n",
        "    if not chat_engine_instance:\n",
        "        yield \"Error: The AI chat engine is not available. Please check the Colab notebook for errors during initialization.\"\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        response_stream = chat_engine_instance.stream_chat(message)\n",
        "        accumulated_response = \"\"\n",
        "        for token in response_stream.response_gen:\n",
        "            accumulated_response += token\n",
        "            yield accumulated_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Gradio query engine processing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        yield f\"Sorry, an error occurred while processing your request: {str(e)}\"\n",
        "\n",
        "# --- Pre-initialize the AI system before launching the UI ---\n",
        "# (This part remains the same as before)\n",
        "print(\"Pre-initializing AI system for Gradio Interface... This might take a few minutes if it's the first run in this session.\")\n",
        "engine_instance = initialize_ai_system()\n",
        "\n",
        "if engine_instance is None:\n",
        "    print(\"CRITICAL ERROR: Could not initialize AI system for Gradio. The UI cannot be launched reliably.\")\n",
        "else:\n",
        "    print(\"AI system pre-initialized successfully and is cached.\")\n",
        "\n",
        "    title = \"AI Research Assistant: CORD-19 & Smoking Linkages\"\n",
        "\n",
        "    iface = gr.ChatInterface(\n",
        "        fn=chat_response_streaming,\n",
        "        title=title,\n",
        "        description=\"Ask questions about the relationship between COVID-19 and smoking, based on CORD-19 dataset. Powered by a Llama 3 8B model.\",\n",
        "        examples=[\n",
        "            [\"What is the link between smoking and COVID-19 severity?\"],\n",
        "            [\"Does vaping affect COVID-19 outcomes?\"],\n",
        "            [\"Are smokers more susceptible to COVID-19?\"]\n",
        "        ],\n",
        "        chatbot=gr.Chatbot(height=600, label=\"Chat Conversation\"),\n",
        "        textbox=gr.Textbox(placeholder=\"Type your question here and press Enter...\", container=False, scale=7, label=\"Your Question\")\n",
        "    )\n",
        "\n",
        "    print(\"\\nLaunching Gradio Interface... Please wait for the public URL.\")\n",
        "    iface.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
